# 第10课：彻底实战详解使用Java开发Spark程序

标签： sparkIMF

---

Hadoop+Spark=A winning combination

##使用Eclipse开发Java的Maven项目

 1. 下载好eclipse：http://www.eclipse.org/downloads/
 2. 解压并启动eclipse
 3. 创建Maven工程
 4. 使用maven-archetype-quickstart
 5. 通过Build Path把默认的Java 1.5变成Java 1.8
 6. 配置pom.xml文件添加程序开发时候的相关依赖，并配置具体的build打包的信息

##Maven下的Spark配置
http://maven.outofmemory.cn/org.apache.spark/

```xml
    <dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-core_2.10</artifactId>
    	<version>1.6.0</version>
    </dependency>
    <dependency>
    	<groupId>org.apache.hadoop</groupId>
    	<artifactId>hadoop-client</artifactId>
    	<version>2.6.4</version>
    </dependency>
    <dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-sql_2.10</artifactId>
    	<version>1.6.0</version>
    </dependency>
    <dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-hive_2.10</artifactId>
    	<version>1.6.0</version>
    </dependency>
    <dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-streaming_2.10</artifactId>
    	<version>1.6.0</version>
    </dependency>
    <dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-streaming-kafka_2.10</artifactId>
    	<version>1.6.0</version>
    </dependency>
    <dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-graphx_2.10</artifactId>
    	<version>1.6.0</version>
    </dependency>
```


##SAM转换

##Java开发Spark源码
```java
package com.dt.spark.sparkapps.cores;

import java.util.Arrays;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

/**
 * 使用Java的方式开发进行本地测试Spark的WordCount程序
 * @author lmr
 * 王家林微博：http://weibo.com/ilovepains
 */
public class WordCount {
	public static void main(String[] args) {
		SparkConf conf = new SparkConf().setAppName("Spark WordCount written by Java");
		conf.setMaster("local");
		
		//如果不同的语言具体的类名称不同，如果是Java的话，则为JavaSparkContext
		JavaSparkContext sc = new JavaSparkContext(conf); //其底层实际上就是Scala的SparkContext
		
		JavaRDD<String> lines = sc.textFile("G:\\runtime\\spark-1.6.0\\README.md");
		
		//Java8  lambda
		JavaRDD<String> words = lines.flatMap(line->{return Arrays.asList(line.split(" "));} );
		//Java原始写法
		//如果是Scala的话，由于SAM转换，所以可以写成一行代码
		/*
		lines.flatMap(new FlatMapFunction<String, String>() {
			@Override
			public Iterable<String> call(String line) throws Exception {
				return Arrays.asList(line.split(" "));
			}
		});
		*/
		JavaPairRDD<String,Integer> pairs = words.mapToPair(word->{return new Tuple2<String, Integer>(word, 1);});
		
		JavaPairRDD<String,Integer> wordsCount = pairs.reduceByKey((Integer a,Integer b)->a+b);
		
		//排序
		wordsCount = wordsCount.mapToPair(tuple->new Tuple2(tuple._2,tuple._1));
		wordsCount = wordsCount.sortByKey(false);
		wordsCount = wordsCount.mapToPair(tuple->new Tuple2(tuple._2,tuple._1));
		
		wordsCount.foreach(tuple->{
			System.out.println(tuple._1+":"+tuple._2);
		});
		
		sc.stop();
	}
}
```


