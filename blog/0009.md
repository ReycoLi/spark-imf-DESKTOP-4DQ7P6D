# 第9课：彻底实战详解 IntelliJ IDEA下的Spark程序开发

标签： sparkIMF 快捷键

---

##Idea开发Spark程序

 1. 下载安装包ideaIC-15.0.3.exe或ideaIU-15.0.3.exe
 2. 本地Java 8和Scala 2.10.4软件套件的安装
 3. 为IDEA安装 Scala Plugin（File->Settings->Plugins->Browse repositories->输入Scala->找下载量最多的安装即可）
 4. 创建Scala项目，并指定JDK1.8.x和Scala 2.10.4
 5. 在IDEA中，菜单File->Project Structure来设置工程的Libraries。核心是添加Spark的jar
 6. 添加Spark的jar依赖spark-assembly-1.6.0-hadoop2.6.0.jar
 
运行代码和上节课一样！

###在生产环境下，一定是通过写自动化shell脚本来自动提交程序的。

##为什么不够能在IDE集成开发环境中直接发布Spark程序到集群中

* 内存和core限制，默认情况下Spark程序的Driver会在提交程序的机器上，所以如果在IDE中提交程序的话，那IDE机器就必须非常强大
* Driver要指挥Workers的运行并频繁的发生通讯，如果开发环境IDE和Spark集群不在同样一个网络下，就会出现任务丢失，运行缓慢等多种不必要的问题
* 这是不安全的！

**一般实际情况下会有专门的和Spark集群在同样一个网络环境下的一台机器上做开发和提交程序，这台机器的配置和集群中的Worker基本上是一致的。**

##Spark源码导入
```sh
# Master development branch
git clone git://github.com/apache/spark.git

# 1.6 maintenance branch with stability fixes on top of Spark 1.6.1
git clone git://github.com/apache/spark.git -b branch-1.6
```
推荐使用IDEA，然后使用Maven，也可以选择SBT导入

##IDEA打包程序
* File->Project Structure->Artifacts->Add->Jar->From modules with Dependencies->选择Module，点击OK即可
* 特别注意：把Spark和Scala的jar从Output Layout界面中去掉
* Build->Build Artifacts->选择需要build的jar->Build

##IDEA常用快捷键：

* Ctrl+E        打开最近打开的文件列表
* Ctrl+Shift+N  查找类
* Shift2次      查找文件
* Ctrl+F        查找内容（当前文件）
* Ctrl+R        替换内容（当前文件）
* Ctrl+Shift+F  高级查找内容（目录）
* Ctrl+Shift+R  高级替换内容（目录）
* Shift+F6      重命名
* Ctrl+Shift+C  复制文件路径
* Alt+Insert    打开重写方法浮动窗口
