# 第19课：Spark高级排序彻底解秘

标签： sparkIMF

---

##基础排序算法

###sortByKey

spark源码解读：
```scala
/**
   * Sort the RDD by key, so that each partition contains a sorted range of the elements. Calling
   * `collect` or `save` on the resulting RDD will return or output an ordered list of records
   * (in the `save` case, they will be written to multiple `part-X` files in the filesystem, in
   * order of the keys).
   */
  // TODO: this currently doesn't work on P other than Tuple2!
  def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)
      : RDD[(K, V)] = self.withScope
  {
    val part = new RangePartitioner(numPartitions, self, ascending)
    new ShuffledRDD[K, V, V](self, part)
      .setKeyOrdering(if (ascending) ordering else ordering.reverse)
  }
```
代码实战：
```scala
sc.textFile("G:/runtime/spark-1.6.0/README.md").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_,1).map(pair=>(pair._2,pair._1)).sortByKey(false).map(pair=>(pair._2,pair._1)).collect
```

##二次排序算法
所谓二次排序就是指排序的时候考虑两个维度

排序的Key要实现scala.math.Ordered.Ordered接口和java.io.Serializable接口

###代码实战
SecondarySort.java
```java
package com.dt.spark.sparkapps.cores;

import java.io.Serializable;

import scala.math.Ordered;

/**
 * 自定义二次排序的Key
 * @author DT大数据梦工厂
 * 新浪微博：http://weibo.com/ilovepains
 */
public class SecondarySort implements Ordered<SecondarySort> , Serializable{
	private static final long serialVersionUID = 1L;
	
	/** 需要二次排序的Key */
	private int first;
	/** 需要二次排序的Value */
	private int second;

	/**
	 * 二次排序的公开构造器
	 * @param first
	 * @param second
	 */
	public SecondarySort(int first,int second) {
		this.first = first;
		this.second = second;
	}
	/**
	 * 大于的情况
	 */
	@Override
	public boolean $greater(SecondarySort other) {
		if(this.first>other.getFirst()){
			return true;
		}else if(this.first== other.getFirst() && this.second>other.getSecond()){
			return true;
		}
		return false;
	}
	/**
	 * 大于等于的情况
	 */
	@Override
	public boolean $greater$eq(SecondarySort other) {
		if(this.$greater(other)){
			return true;
		}else if(this.first==other.getFirst() && this.second==other.getSecond()){
			return true;
		}
		return false;
	}
	/**
	 * 小于的情况
	 */
	@Override
	public boolean $less(SecondarySort other) {
		if(this.first<other.getFirst()){
			return true;
		}else if(this.first==other.getFirst() && this.second<other.getSecond()){
			return true;
		}
		return false;
	}
	/**
	 * 小于等于的情况
	 */
	@Override
	public boolean $less$eq(SecondarySort other) {
		if(this.$less(other)){
			return true;
		}else if(this.first==other.getFirst() && this.second==other.getSecond()){
			return true;
		}
		return false;
	}

	@Override
	public int compare(SecondarySort other) {
		if(this.first - other.getFirst() !=0 ){
			return this.first - other.getFirst();
		}else{
			return this.second - other.getSecond();
		}
	}

	@Override
	public int compareTo(SecondarySort other) {
		if(this.first - other.getFirst() !=0 ){
			return this.first - other.getFirst();
		}else{
			return this.second - other.getSecond();
		}
	}

	@Override
	public int hashCode() {
		final int prime = 31;
		int result = 1;
		result = prime * result + first;
		result = prime * result + second;
		return result;
	}
	@Override
	public boolean equals(Object obj) {
		if (this == obj)
			return true;
		if (obj == null)
			return false;
		if (getClass() != obj.getClass())
			return false;
		SecondarySort other = (SecondarySort) obj;
		if (first != other.first)
			return false;
		if (second != other.second)
			return false;
		return true;
	}
	public static long getSerialversionuid() {
		return serialVersionUID;
	}

	public int getFirst() {
		return first;
	}

	public int getSecond() {
		return second;
	}
}
```
SecondarySortApp.java
```java
package com.dt.spark.sparkapps.cores;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.PairFunction;

import scala.Tuple2;

/**
 * 而次排序，具体的实现步骤：
 * 第一步：按照Ordered和Serializable接口实现自定义排序的Key
 * 第二步：将要进行二次排序的文件加载进来生成<Key,Value>类型的RDD
 * 第三步：使用sortByKey基于自定义的Key进行而次排序
 * 第四步：去除掉排序的Key，只保留排序的结果
 * @author DT大数据梦工厂
 * 新浪微博：http://weibo.com/ilovepains
 */
public class SecondarySortApp {
	public static void main(String[] args) {
		
		JavaSparkContext sc = new JavaSparkContext(new SparkConf().setAppName("SecondarySortApp").setMaster("local"));
		
		JavaRDD<String> lines = sc.textFile("G:/txt/test/testSort.txt");
		
		JavaPairRDD<SecondarySort, String> pairs = lines.mapToPair(new PairFunction<String, SecondarySort, String>() {
			@Override
			public Tuple2<SecondarySort, String> call(String line) throws Exception {
				String[] splited = line.split(" ");
				SecondarySort key = new SecondarySort(Integer.parseInt(splited[0]), Integer.parseInt(splited[1]));
				return new Tuple2<SecondarySort, String>(key,line);
			}
		});
		JavaPairRDD<SecondarySort, String> sorted = pairs.sortByKey();	//完成二次排序
		
		//过滤掉排序后自定义的Key，保留排序的结果
		JavaRDD<String> secondarySorted = sorted.map(new Function<Tuple2<SecondarySort,String>, String>() {
			@Override
			public String call(Tuple2<SecondarySort, String> sortedContent) throws Exception {
				return sortedContent._2;
			}
		});
		
		secondarySorted.collect().forEach(line->System.out.println(line));
		
	}
}
```
文件内容：
```text
2 3
4 1
3 2
4 3
8 7
2 1
```
结果：
```java
2 1
2 3
3 2
4 1
4 3
8 7
```

###scala的代码实现
```scala
package com.dt.spark.sparkapps

import org.apache.spark.{SparkConf, SparkContext}

/**
 * 自定义二次排序的Key
 * @author DT大数据梦工厂
 * 新浪微博：http://weibo.com/ilovepains
 * Created by Limaoran on 2016/5/11.
 */
class SecondarySort (val first:Int,val second:Int) extends Ordered[SecondarySort] with Serializable{
  override def compare(that: SecondarySort): Int = {
    if(this.first - that.first != 0){
      return this.first - that.first
    }else {
      return this.second - that.second
    }
    return 0
  }
}
/**
 * 而次排序，具体的实现步骤：
 * 第一步：按照Ordered和Serializable接口实现自定义排序的Key
 * 第二步：将要进行二次排序的文件加载进来生成<Key,Value>类型的RDD
 * 第三步：使用sortByKey基于自定义的Key进行而次排序
 * 第四步：去除掉排序的Key，只保留排序的结果
 */
object SecondarySort{
  def apply(first:String,second:String)={
    new SecondarySort(first.toInt,second.toInt)
  }

  def main(args: Array[String]) {
    val sc = new SparkContext(new SparkConf().setAppName("SecondarySortApp").setMaster("local"))

    val lines = sc.textFile("G:/txt/test/testSort.txt")

    lines.map(line=>{
      val splited = line.split(" ")
      (SecondarySort(splited(0),splited(1)),line)
    }).sortByKey().map(pair=>pair._2).collect().foreach(println)

  }
}
```

##更高级别排序算法



