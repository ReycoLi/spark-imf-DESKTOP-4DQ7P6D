# 第34课：Stage划分和Task最佳位置算法源码彻底解密

标签： sparkIMF

---


##一：Stage划分算法解密

 1. Spark Application中可以因为不同的Action触发众多的Job，也就是说一个Application中可以有很多的Job，每个Job是由一个或者多个Stage构成的，后面的Stage依赖于前面的Stage，也就是说只有前面依赖的Stage计算完毕后，后面的Stage才会运行。
 2. Stage划分的依据就是宽依赖，什么时候产生宽依赖呢？例如reduceByKey、groupByKey等等。
 3. 由Action（例如collect）导致了SparkContext.runJob的执行，最终导致了DAGScheduler中的submitJob的执行，其核心是通过发送一个case class JobSubmitted对象给eventProcessloop，其中JobSubmitted源码如下：
    ```scala
    private[scheduler] case class JobSubmitted(
      jobId: Int,
      finalRDD: RDD[_],
      func: (TaskContext, Iterator[_]) => _,
      partitions: Array[Int],
      callSite: CallSite,
      listener: JobListener,
      properties: Properties = null)
    extends DAGSchedulerEvent
    ```
    
    eventProcessLoop是DAGSchedulerEventProcessLoop的具体实例，而DAGSchedulerEventProcessLoop是EventLoop的子类，具体实现EventLoop的onReceive方法，onReceive方法转过来回调doOnReceive。
    
    DAGSchedulerEventProcessLoop里面开辟了一条线程，这个线程不断的循环一个队列，post的时候其实就是把消息放到队列中，由于把消息放到队列，它在不断的循环所以就可以拿到这个消息，拿到这个消息后，调用回调方法onReceive。
    
 4. 在doOnReceive中通过模式匹配的方式把执行路由到
    ```scala
    case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>
      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)
    ```
 5. 在handleJobSubmitted中首先创建finalStage，创建finalStage时候会建立父Stage的依赖链条。

补充说明：所谓的missting就是说要进行当前的计算了。

###第一点：它为什么开辟一条线程？

支持多个Job提交，异步处理多Job。

###另外一点：它现在是自己给自己发消息，它要这么搞，其实看上去有点多此一举，但是它为什么还要这么做呢？

这里面蕴含了一个非常非常重要的理念，这个理念就是：如果说无论是你自己发消息，还是说别人发消息，你都采用一个消息循环线程去处理的话，这个时候大家处理的方式就是统一的，你的逻辑思路就是一致的，这样你的扩展性就会非常非常的好。其实转过来写代码的时候，代码也会写的非常非常的干净。因为我们写代码一般是分工协作的。


##二：Task任务本地性算法实现

 1. 在submitMissingTasks中会通过调用以下代码来获得任务的本地性：
    ```scala
    val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try {
      stage match {
        case s: ShuffleMapStage =>
          partitionsToCompute.map { id => (id, getPreferredLocs(stage.rdd, id))}.toMap
        case s: ResultStage =>
          val job = s.activeJob.get
          partitionsToCompute.map { id =>
            val p = s.partitions(id)
            (id, getPreferredLocs(stage.rdd, p))
          }.toMap
      }
    }
    ```
    
 2. 具体一个Partition中的数据本地性的算法实现位于下述代码中：
    ```scala
    private[spark]
    def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = {
      getPreferredLocsInternal(rdd, partition, new HashSet)
    }
    ```
    
    在具体算法实现的时候首先查询DAGScheduler的内存数据结构中是否存在当前Partition的数据本地性的信息，如果有的话则直接返回；如果没有首先会调用rdd.getPreferedLocaltions。
    例如想让Spark运行在HBase上或者一种现在还没有直接支持的数据库上面，此时开发者需要自定义RDD，为了保证Task计算的数据本地性，最为关键的方法就是必须实现RDD的getPreferedLocaltions。
 3. DAGScheduler计算数据本地性的时候巧妙的借助了RDD自身的getPreferedLocations中的数据，最大化的优化了效率，因为getPreferedLocation中表明了每个Partition的数据本地性，虽然当前Partition可能被persist或者checkpoint，但是persist或者checkpoint默认情况下肯定是和getPreferedLocation中的Partition的数据本地性是一致的，所以这就极大的简化了Task数据本地性算法的实现和效率的优化。
