# 第15课：RDD创建内幕彻底解密

标签： sparkIMF

---

Hadoop + Spark = 目前大数据领域最有前途的组合

你可以在智能设备 例如手机、平板、电视上使用Spark，也可以在PC和Server上使用Spark

Local模式：默认情况下如果失败了，就是失败了

Hadoop的MapReduce现在没有任何使用场景

###实际上Spark的并行度到底应该设置为多少呢？
    最佳实践：每个core可以承载2-4个partition。并行度跟数据规模没关系，只跟每个Task在计算partition时候的内存使用量和CPU使用时间有关系。

##RDD的创建方式

第一个RDD：代表了Spark应用程序输入数据的来源
通过Transformation来对RDD进行各种算子的转换
实现算法

 1. 使用程序中的集合创建RDD
 2. 使用本地文件系统创建RDD
 3. 使用HDFS创建RDD
 4. 基于DB创建RDD
 5. 基于NoSQL，例如HBase
 6. 基于S3创建RDD
 7. 基于数据流创建RDD

##RDD创建实战

 1，通过集合创建RDD的实际意义：测试！
 
```scala
package com.dt.spark.sparkapps
import org.apache.spark.{SparkConf, SparkContext}
/**
 * 计算1-100的和
 * Created by Limaoran on 2016/5/10.
 */
object RDDBasedOnCollections {
  def main(args:Array[String]): Unit ={
    //创建一个Scala集合
    val collection = 1 to 100

    val sc = new SparkContext(new SparkConf().setAppName("RDDBasedOnCollections").setMaster("local"))

    val rdd = sc.parallelize(collection)

    val sum = rdd.reduce(_+_) //1+2=3 3+3=6 6+4=10  10+5=15

    println("1+2+...+99+100="+sum)

    sc.stop()
  }
}
```

2，使用本地文件系统创建RDD的作用：测试大量数据的文件

```scala
package com.dt.spark.sparkapps
import org.apache.spark.{SparkConf, SparkContext}
/**
 * 计算文件所有行的长度总和
  * Created by Limaoran on 2016/5/10.
  */
object RDDBasedOnLocalFile {
   def main(args:Array[String]): Unit ={
     val sc = new SparkContext(new SparkConf().setAppName("RDDBasedOnCollections").setMaster("local"))

     val rdd = sc.textFile("G:\\runtime\\spark-1.6.0\\README.md")
     val sum = rdd.map(line=>line.length).reduce(_+_)

     println("sum="+sum)

     sc.stop()
   }
}
```
 3，使用HDFS来创建RDD：生产环境最长用的RDD创建方式
也是使用textFile读取HDFS上的数据 


##数据本地性
数据库中的数据，Tachyon会帮你全部要抽取下来，或者把数据导入到HDFS或Hive上，让Spark在Hive上运行。或者说你数据库的集群上安装有Spark，Spark直接在你数据库所在的机器上运行。

