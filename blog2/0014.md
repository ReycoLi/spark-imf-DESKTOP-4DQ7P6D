# 第14课：Spark RDD解密

标签： sparkIMF

---

精通了RDD，学习Spark时间将会大大的缩短，遇到问题的时候，解决问题的能力大大的提高。

###学习Spark的捷径：彻底把精力首先聚焦在RDD的理解上
> RDD本身提供了通用的抽象
 RDD有5个子框架，可以根据其他领域的内容进行建模做另一个库

###所有顶级的Spark高手一定有两点特征：
 1. 能够解决Spark的各种bug和性能调优
 2. 一般拿Spark过来就是做修改的，以配合自己所从事领域的具体的业务

基于工作集的应用抽象


##Hadoop的MapReduce是基于数据集的

基于数据集的处理：从物理存储上加载数据，然后操作数据，然后写入物理存储设备

###基于数据集的操作不适应的场景：
 1. 不适合于大量的迭代
 2. 交互式查询
重点是：基于数据流的方式不能够复用曾经的结果或者中间计算结果

##Spark RDD是基于工作集的

工作流和工作集的共同特点：位置感知、自动容错、负载均衡

位置感知：Spark在进行Partitioner进行下一个Stage操作的时候会确定位置，它是更精致化的。

RDD：Resilient Distributed DataSet

##RDD 弹性分布式数据集

 1. 自动的进行内存和磁盘数据存储的切换
 2. 基于Lineage的高效容错
 3. Task如果失败会自动进行特定次数的重试
 4. Stage如果失败会自动进行特定次数的重试，当Task或Stage失败时，只计算失败的数据分片，如果超出次数还是失败，则全部失败！
 5. checkpoint和persist
 6. 数据调度弹性：DAG、TASK和资源管理无关
 7. 数据分片的高度弹性repartition
所以把很多更多的数据分片合并成小的数据分片的时候，千万不要直接调用repartition，要调用coalesce
```scala
def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    coalesce(numPartitions, shuffle = true)
  }
```
```scala
def coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null)
      : RDD[T] = withScope {...}
```
如果说把1万个数据分片变为10万个数据分片的话，这个时候可以用shuffle，也可以不用shuffle。

**RDD本身允许用户在多个查询时显示的将工作集缓存在内存中，那我们以后其他的人来查询的话，就可以重用我们工作集。这就可以极大的提升查询速度。**

工作集指两个层面：曾经运行的工作，其他的人再运行的话不需要重新运行，而基于数据流则需要重新运行。


RDD依赖的清除：checkpoint

如果一个Stage有1000RDD，默认情况下只产生一次结果。

RDD是分布式函数式编程的抽象
RDD的核心之一就是lazy

Spark用于生产环境，最低版本1.3，因为有DataFrame

常规容错的方式：数据检查点和记录数据的更新

RDD通过数据记录更新的方式为何很高效？

 1. RDD是不可变的+Lazy
> Lazy级别的，且是不可变的，构成了链条，导致了2点结果：第一点计算的时候从后往前回溯，它不会每次都产生中间的计算结果，它只是记录了进行哪些操作。第二点容错的时候它会记录前面的东西，在前面某步骤的基础上而不是从头开始算。

 2. RDD是粗粒度的操作
> 所谓粗粒度就是每次操作的时候就作用到的所有数据集合。
> RDD的写操作是粗粒度的，但是RDD的读操作既可以是粗粒度的也可以是细粒度的。

 3. 

RDD的一系列的数据分片上面运行的计算逻辑都是一样的。

##Spark要统一数据计算领域，除了实时事务性处理

实时事务性处理取代不了

