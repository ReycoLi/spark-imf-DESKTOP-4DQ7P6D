# 第70课：Spark SQL内置函数解密与实战

标签： sparkIMF

---

##内置函数

数据库例如说Oracle、MySQL有很多内置函数，而且内置函数的多少以及内置函数的质量往往也是我们对一款数据处理产品一个非常重要的评价的指标。
Spark SQL的内部也引入了大量的内部函数，而且这些内部函数一般情况表都会使用了CG（Code Generation 代码自动生成）功能，这样也就导致了内置函数一般在编译和执行的时候都是经过高度优化的！

[DataFrame API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame)


##内置函数

* agg：aggregate 聚合函数，聚合一般是基于groupBy的基础之上

```scala
val people = sqlContext.read.parquet("...")
val department = sqlContext.read.parquet("...")

people.filter("age > 30")
  .join(department, people("deptId") === department("id"))
  .groupBy(department("name"), "gender")
  .agg(avg(people("salary")), max(people("age")))
```

* asin： 反正弦
* atan：反正切
* sqrt：平方根；开方
* tan：正切
* round

开窗函数，如果企业注重实战，注重你的技术的话，一定会问你的！

**所有的内置函数操作都会返回具体的列！**这个列和我们已有的DataFrame中的列没有什么区别，而且DataFrame的列可以动态生长， 
* 内置函数返回的是Column对象；
* 内置函数底层会采用CG的功能；
* 作为一个真正成熟的开发者，一定会用agg()的！

##代码实战：

SparkSQLAgg.scala
```scala
package com.dt.spark.sparkapps.sql

import org.apache.spark.sql.{Row}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkContext, SparkConf}

/**
 * 第70课：Spark SQL中的内置函数对数据进行分析，与普通的Spark SQL API不同的是，
 *  DataFrame内置函数操作的结果是返回一个Column对象，而DataFrame天生就是
 *  “A distributed collection of data organized into named columns.” 分布式的面向列的数据集合。
 *  这就为数据的复杂分析建立了坚实的基础，并提供了极大的方便性，例如说，我们在操作
 *  DataFrame的方法中可以随时调用内置函数进行业务需要的处理，这之于我们构建复杂的业务逻辑而言
 *  是可以极大的减少不必要的时间消耗（基本上就是实际模型的映射），让我们聚焦在数据分析上，
 *  这对于提高工程师的生产力而言是非常有价值的。
 *  在Spark 1.5.x开始提供了大量的内置函数，例如：agg
 *  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
 *    groupBy().agg(aggExpr, aggExprs : _*)
 *  }
 *  还有max、mean、min、sum、avg、explode、size、sort_array、day、todate、abs、acros、asin、atan
 *  总体上而言内置函数包含了五大基本类型：
 *    1、聚合函数，例如：countDistinct、sumDistinct等
 *    2、集合函数，例如：sort_array、explode等
 *    3、日期、时间函数，例如：hour、quarter、next_day等
 *    4、数学函数，例如：asin、atan、sqrt、tan、round等
 *    5、开窗函数，例如：rowNumber等
 *    6、字符串函数，例如：concat、format_number、regexp_extract
 *    7,、其他函数，例如：isNaN、sha、randn、callUDF、callUDAF
 * Created by Limaoran on 2016/7/5.
 */
object SparkSQLAgg {
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("SparkSQLInnerFunctions").setMaster("local")
    val sc=  new SparkContext(conf)
    val sqlContext = new HiveContext(sc)  //构建SQL上下文

    /**
     * 第三步：模拟电商访问的数据，实际情况会比模拟数据复杂很多，最后生成RDD
     */
    val userData = Array[String](
      "20160327,001,http://spark.apache.org,1000",
      "20160327,001,http://hadoop.apache.org,1100",
      "20160327,002,http://flink.apache.org,1300",
      "20160328,003,http://kafka.apache.org,1200",
      "20160328,002,http://hive.apache.org,1500",
      "20160328,001,http://parquet.apache.org,1800"
    )
    //生成RDD分布式集合对象
    val userDataRDD = sc.parallelize(userData)

    /**
     * 第四步：根据业务需要对数据进行预处理生成DataFrame，要想把RDD转换成DataFrame，
     * 需要先把RDD中的元素类型变成Row类型，与此同时要提供DataFrame中的Columns的元数据信息描述
     */
    val userDataRDDRow = userDataRDD.map(data=>{
      val splited = data.split(",")
      Row(splited(0),splited(1),splited(2),splited(3).toInt)
    })
    import org.apache.spark.sql.types._
    val structType = StructType(Array(
      StructField("time",StringType,true),
      StructField("id",StringType,true),
      StructField("url",StringType,true),
      StructField("amount",IntegerType,true)
    ))

    val userDataDF = sqlContext.createDataFrame(userDataRDDRow,structType)

    /**
     * 第五步：使用Spark SQL提供的内置函数对DataFrame进行操作，特别注意：内置函数生成的Column对象且自动进行CG。
     *
     */
    //导入Spark SQL的内置转换函数
    import org.apache.spark.sql.functions._
    //要使用Spark SQL的内置函数，就一定要导入SQLContext下的隐式转换
    import sqlContext.implicits._

    //此时使用了'time和"time"做了测试，效果一样？
    //每一天有多少用户访问，执行结果：
    //  [20160327,2]
    //  [20160328,3]
    userDataDF.groupBy('time).agg('time,countDistinct('id)).map(row=>{
      Row(row(1),row(2))
    }).collect().foreach(println)

    //获取每一天的收入情况，执行结果：
    //|    time|    time|sum(amount)|
    //|20160327|20160327|       3400|
    //|20160328|20160328|       4500|
    userDataDF.groupBy('time).agg('time,sum('amount)).show()
  }
}
```

执行结果：

1.每个日期有多少用户访问？

```text
[20160327,2]
[20160328,3]
```

2.获取每一天的收入金额

```text
+--------+--------+-----------+
|    time|    time|sum(amount)|
+--------+--------+-----------+
|20160327|20160327|       3400|
|20160328|20160328|       4500|
+--------+--------+-----------+
```


##Spark SQL on Hive和Hive on Spark区别

Spark SQL操作Hive上的数据，这叫Spark on Hive；
Hive on Spark是指依旧以Hive为核心，但是引擎从MapReduce换成了Spark。


##作业：自己去研究一下agg内置函数到底是怎么工作的？
