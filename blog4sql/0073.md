# 第73课：Spark SQL Thrift Server实战

标签： sparkIMF

---

Thrift是一个C/S通信模型的一种协议！

我们可以通过Thrift Server可以从web的角度访问Spark SQL，或者说通过JDBC、ODBC来访问Thrift Server，然后通过Thrift Server访问Spark SQL！然后通过Spark SQL访问Hive的数据仓库，这是个非常经典的过程。

**通过JDBC/ODBC->Thrift Server->Spark SQL->Hive 这个在企业中是一种非常经典的架构，这个架构如果你对实时性要求不是特别高的情况下，一般而言它可以取代传统的以数据库为后台的数据处理的系统。**

[Spark 官方文档参考 Distributed SQL Engine ](http://spark.apache.org/docs/latest/sql-programming-guide.html#distributed-sql-engine)

##启动Thrift Server

###linux系统下启动脚本
```sh
./sbin/start-thriftserver.sh
```
为了方便测试，可以使用local模式运行
```sh
./sbin/start-thriftserver.sh --master local
```

###Windows系统下thrift-server启动脚本

```sh
java -cp G:\runtime\spark-1.6.0/conf\;G:/runtime/spark-1.6.0/lib/spark-assembly-1.6.0-hadoop2.6.0.jar;G:\runtime\spark-1.6.0\lib\datanucleus-api-jdo-3.2.6.jar;G:\runtime\spark-1.6.0\lib\datanucleus-core-3.2.10.jar;G:\runtime\spark-1.6.0\lib\datanucleus-rdbms-3.2.9.jar;G:/runtime/hadoop-2.6.4/etc/hadoop\ -Xms1g -Xmx1g org.apache.spark.deploy.SparkSubmit --master local --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 spark-internal
```
可以添--master local参数在本地模式下运行thrift server！

##使用beeline测试连接，操作Thrift Server

* 启动beeline
* 输入 !connect jdbc:hive2://localhost:10000
* 输入用户名、密码，比如用户名root密码root即可！
* 此时观察是否能连接成功：
```sh
root@Master:~# beeline
Beeline version 1.6.0 by Apache Hive
beeline> !connect jdbc:hive2://localhost:10000
Connecting to jdbc:hive2://localhost:10000
Enter username for jdbc:hive2://localhost:10000: root
Enter password for jdbc:hive2://localhost:10000: ****
16/07/06 18:14:41 INFO Utils: Supplied authorities: localhost:10000
16/07/06 18:14:41 INFO Utils: Resolved authority: localhost:10000
16/07/06 18:14:41 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:10000
Connected to: Spark SQL (version 1.6.0)
Driver: Spark Project Core (version 1.6.0)
Transaction isolation: TRANSACTION_REPEATABLE_READ
```
* show databases;
* use hive;
* show tables;
* desc sogouq;
* select count(*) from  sogouq;


##源码阅读

* HiveThriftServer2

##代码实战

代码中使用的是http连接方式，所以启动thrift-server的脚本有变化：
```sh
启动thriftserver：start-thriftserver.sh --master local --hiveconf hive.server2.transport.mode=http --hiveconf hive.server2.thrift.http.path=cliservice
```
Win系统下完整脚本：
```sh
java -cp G:\runtime\spark-1.6.0/conf\;G:/runtime/spark-1.6.0/lib/spark-assembly-1.6.0-hadoop2.6.0.jar;G:\runtime\spark-1.6.0\lib\datanucleus-api-jdo-3.2.6.jar;G:\runtime\spark-1.6.0\lib\datanucleus-core-3.2.10.jar;G:\runtime\spark-1.6.0\lib\datanucleus-rdbms-3.2.9.jar;G:/runtime/hadoop-2.6.4/etc/hadoop\ -Xms1g -Xmx1g org.apache.spark.deploy.SparkSubmit --master local --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 spark-internal  --hiveconf hive.server2.transport.mode=http --hiveconf hive.server2.thrift.http.path=cliservice
```

SparkSQLJDBC2ThriftServer.java
```java
package com.dtspark.sparkapps.sql;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.ResultSet;

/**
 * 第73课：实战演示Java通过JDBC访问Thrift Server，进而访问Spark SQL，进而访问Hive，这是企业级开发中最为常见的一种方式。
 * Created by Limaoran on 2016/7/6.
 */
public class SparkSQLJDBC2ThriftServer {
    public static void main(String[] args) {
        String sql = "select * from people where age>?";
        try {
            Class.forName("org.apache.hive.jdbc.HiveDriver");
//            new org.apache.hive.jdbc.HiveDriver();
            //1、默认连接方式：
            // DriverManager.getConnection("jdbc:hive2://MasterWin:10000/hive","root","root")
            //2、http连接方式：
            //启动thriftserver：start-thriftserver.sh --master local --hiveconf hive.server2.transport.mode=http --hiveconf hive.server2.thrift.http.path=cliservice
            //beeline> !connect jdbc:hive2://<host>:<port>/<database>?
            // hive.server2.transport.mode=http;hive.server2.thrift.http.path=<http_endpoint>
            try(Connection con = DriverManager.getConnection("jdbc:hive2://MasterWin:10001/hive" +
                    "?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice","root","root")){
                con.createStatement().execute("use hive");
                PreparedStatement pst = con.prepareStatement(sql);
                pst.setInt(1,20);
                ResultSet rs = pst.executeQuery();
                while(rs.next()){
                    System.out.println(rs.getString(1)+","+rs.getInt(2));
                }
            }catch (Exception ex){
                ex.printStackTrace();
            }
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
        }
    }
}
```
