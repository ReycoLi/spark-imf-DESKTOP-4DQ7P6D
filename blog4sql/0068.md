# 第68课：Spark SQL通过JDBC操作Mysql

标签： sparkIMF

---

##一：使用Spark通过JDBC操作数据库

Spark SQL可以通过JDBC从传统的关系型数据库中读写数据，读取数据后直接生成的是DataFrame，然后再加上借助于Spark内核的丰富的API来进行各种操作；

##二：代码实战

###Java版

SparkSQLJDBC2MySQL.java

```java
package com.dtspark.sparkapps.sql;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.sql.*;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import scala.Tuple2;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.Statement;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;

/**
 * 第68课：Spark SQL通过JDBC操作MySQL
 * Created by Limaoran on 2016/7/5.
 */
public class SparkSQLJDBC2MySQL {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("SparkSQLJDBC2MySQL");
        conf.setMaster("local");
        JavaSparkContext sc = new JavaSparkContext(conf);
        SQLContext sqlContext = new SQLContext(sc);

        /**
         * 1，通过format("jdbc")的方式说明Spark SQL操作的数据来源是通过JDBC获得的，
         *      JDBC后端一般都是数据库，例如MySQL、Oracle等。
         * 2，通过DataFrameReader的Option方法把要访问的数据库的信息传递进去
         *      url:代表数据库的jdbc连接地址。
         *      dbtable：具体要连接使用哪个数据库。
         * 3，Driver部分是Spark SQL访问数据库的具体的驱动的完整包名和类名。
         * 4，关于JDBC的驱动的jar，可以放在Spark的lib目录中，也可以在使用spark-submit的时候指定具体的jar。
         *      （编码和打包的时候都不需要这个JDBC的jar）
         */
        DataFrameReader reader = sqlContext.read().format("jdbc");
        reader.option("url","jdbc:mysql://MasterWin:3306/spark");
        reader.option("dbtable","dtspark");
        reader.option("driver","com.mysql.jdbc.Driver");
        reader.option("user","root");
        reader.option("password","root");

        /**
         * 在实际的企业级开发环境中，如果数据库中数据规模特别大，
         *      例如说10亿条数据，此时采用传统的DB去处理的话一般需要对10亿条数据分成很多批次处理，
         *      例如分成100批（受限于单台Server的处理能力），且实际的处理过程可能会非常复杂，
         *      通过传统的Java EE等技术可能很难或者不方便实现处理算法，此时采用Spark SQL获取数据库中的数据
         *      并进行分布式处理就可以非常好的解决该问题，但是由于Spark SQL加载DB中的数据需要时间，
         *      所以一般会在Spark SQL和具体要操作的DB之间加上一个缓冲层次。
         *      例如中间使用Redis，可以把Spark处理速度提高到甚至45倍。
         *
         */
        DataFrame dtsparkDataSourceDF = reader.load();//基于dtspark表创建DataFrame

        reader.option("dbtable","dthadoop");
        DataFrame dthadoopDataSourceDF = reader.load();//基于dthadoop表创建DataFrame

        /**
         * 把DataFrame转换成为RDD，并且基于RDD进行join操作
         */
        JavaPairRDD<String,Tuple2<Integer,Integer>> resultRDD = dtsparkDataSourceDF.javaRDD().mapToPair(new PairFunction<Row, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(Row row) throws Exception {
                return new Tuple2(row.getString(row.fieldIndex("name")),row.getInt(row.fieldIndex("age")));
            }
        }).join(dthadoopDataSourceDF.javaRDD().mapToPair(new PairFunction<Row, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(Row row) throws Exception {
                return new Tuple2(row.getString(row.fieldIndex("name")),row.getInt(row.fieldIndex("score")));
            }
        }));

        JavaRDD<Row> resultRowRDD = resultRDD.map(new Function<Tuple2<String,Tuple2<Integer,Integer>>, Row>() {
            @Override
            public Row call(Tuple2<String, Tuple2<Integer, Integer>> tuple) throws Exception {
                return RowFactory.create(tuple._1(),tuple._2()._2(),tuple._2()._1());
            }
        });

        List<StructField> structFileds = new ArrayList<>();
        structFileds.add(DataTypes.createStructField("name", DataTypes.StringType, false));
        structFileds.add(DataTypes.createStructField("age", DataTypes.IntegerType, true));
        structFileds.add(DataTypes.createStructField("score", DataTypes.IntegerType, true));
        StructType structType = DataTypes.createStructType(structFileds);

        DataFrame personsDF = sqlContext.createDataFrame(resultRowRDD,structType);

        personsDF.show();

        /**
         * 1，当DataFrame要把通过Spark SQL、Core、ML等复杂操作后的数据写入数据库的时候
         *      首先是权限问题，必须确保数据库授权了当前操作Spark SQL的用户；
         * 2，DataFrame要写数据到DB的时候一般都不可以直接写进去，而是要转成RDD，通过RDD写数据到DB中。
         *
         * 在把处理后的结果写入数据库的时候，要注意：
         *  使用foreachPartition，之所以采用foreachPartition，你如果不采用的话，
         *      你每个Partition中的每一条记录，都要连一次数据库，那这个数据库连接句柄就崩溃了！
         *      所以我们这边每一个Partition连一次，Partition内部连上之后里面的每一条记录加进去！当然也可以采用Batch的方式！
         */
        personsDF.javaRDD().foreachPartition(new VoidFunction<Iterator<Row>>() {
            @Override
            public void call(Iterator<Row> rowIterator) throws Exception {
                try (Connection con = DriverManager.getConnection("jdbc:mysql://MasterWin:3306/spark", "root", "root");) {
                    Statement st = con.createStatement();
                    while(rowIterator.hasNext()){
                        Row row = rowIterator.next();
                        //注意添加Batch的时候，sql语句后面不能有分号“;”
                        st.addBatch("insert into dtresult(name,age,score) values ('"+row.getAs("name")+"',"+row.getAs("age")+","+row.getAs("score")+")");
                    }
                    st.executeBatch();
                }
            }
        } );

        sc.close();
        }
    }

```

<font color='red'>你要写出算法实现很好、效率又很高的东西，你基本上都是我这样写的方式，你不要总是想着用高层API，高层API就是给入门级别的人用的，所以一出现性能问题，那些人就搞不定了！</font>

###Scala版

SparkSQLJDBC2MySQL.java
```scala
package com.dt.spark.sparkapps.sql

import java.sql.{SQLException, DriverManager}
import java.util.Properties

import org.apache.spark.sql.types.{DataTypes, StructField, StructType}
import org.apache.spark.sql.{Row, SQLContext}
import org.apache.spark.{SparkContext, SparkConf}


/**
 * 第68课：Spark SQL通过JDBC操作MySQL
 * Created by Limaoran on 2016/7/5.
 */
object SparkSQLJDBC2MySQL {
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("SparkSQLJDBC2MySQLScala").setMaster("local")
    val sc = new SparkContext(conf)
    val sqlContext = new SQLContext(sc)

    val reader = sqlContext.read.option("url","jdbc:mysql://MasterWin:3306/spark").
      option("driver","com.mysql.jdbc.Driver").
      option("user","root").option("password","root").option("dbtable","dtspark").format("jdbc")
    val dtsparkDataDF = reader.load()

    val dthadoopDataDF = reader.option("dbtable","dthadoop").load()

    //join
    val resultRDD = dtsparkDataDF.rdd.map(row=>Tuple2(row.getString(row.fieldIndex("name")),row.getInt(row.fieldIndex("age")))).join(dthadoopDataDF.rdd.map(row=>Tuple2(row.getString(row.fieldIndex("name")),row.getInt(row.fieldIndex("score")))))

    val resultRow = resultRDD.map(tuple=>Row(tuple._1,tuple._2._1,tuple._2._2))
    val peopleDF = sqlContext.createDataFrame(resultRow,StructType(Array[StructField](StructField("name",DataTypes.StringType,false),
      StructField("age",DataTypes.IntegerType),StructField("score",DataTypes.IntegerType))))

    peopleDF.show()

    peopleDF.foreachPartition(row=>{
      val con = DriverManager.getConnection("jdbc:mysql://localhost:3306/spark","root","root");
      try{
        val st = con.createStatement()
        row.foreach(r=>{
          st.addBatch("insert into dtresult(name,age,score) values('"+r.getAs("name")+"',"+r.getAs("age")+","+r.getAs("score")+")")
        })
        st.executeBatch()
      }catch {
        case e:SQLException=>e.printStackTrace()
        case allE:Exception=>allE.printStackTrace()
      }finally {
        if(con!=null ) con.close()
      }
    })

  }
}
```

###测试数据

使用数据库MySQL，连接数据库名称：spark

```sql
DROP TABLE IF EXISTS `dthadoop`;
CREATE TABLE `dthadoop` (
  `name` varchar(32) NOT NULL,
  `score` int(3) default NULL,
  PRIMARY KEY  (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

INSERT INTO `dthadoop` VALUES ('flume', '92');
INSERT INTO `dthadoop` VALUES ('hadoop', '65');
INSERT INTO `dthadoop` VALUES ('spark', '99');

DROP TABLE IF EXISTS `dtresult`;
CREATE TABLE `dtresult` (
  `name` varchar(32) NOT NULL,
  `age` int(3) default NULL,
  `score` int(3) default NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

INSERT INTO `dtresult` VALUES ('spark', '6', '99');
INSERT INTO `dtresult` VALUES ('hadoop', '11', '65');
INSERT INTO `dtresult` VALUES ('flume', '5', '92');

DROP TABLE IF EXISTS `dtspark`;
CREATE TABLE `dtspark` (
  `name` varchar(32) NOT NULL,
  `age` int(3) default NULL,
  PRIMARY KEY  (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

```