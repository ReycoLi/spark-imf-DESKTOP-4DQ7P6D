# 第61课：Spark SQL数据加载和保存内幕深度解密实战

标签： sparkIMF

---

##阅读源码：

* SQLContext
* DataFrameReader
* ResolvedDataSource
* DataFrame
* DataFrameWriter

##代码实战

###json格式

SparkSQLLoadSaveOps.java

```java
package com.dtspark.sparkapps.sql;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.sql.*;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import java.util.ArrayList;
import java.util.List;

/**
 * 第61课：Spark SQL数据加载和保存实战
 * Created by Limaoran on 2016/7/2.
 */
public class SparkSQLLoadSaveOps {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("SparkSQLLoadSaveOps");
        conf.setMaster("local");
        JavaSparkContext sc = new JavaSparkContext(conf);
        SQLContext sqlContext = new SQLContext(sc);

        DataFrame peopleDF = sqlContext.read().format("json").load("G:\\runtime\\spark-1.6.0\\examples\\src\\main\\resources\\people.json");

        // mode设置写入模式：覆盖、追加、忽略；     json内部调用 .format("json").save(path)
        peopleDF.select("name").write().mode(SaveMode.Append).json("z:/peopleNames.json");

        sc.close();
    }
}
```

###parquet格式

无论是Hadoop还是说Spark，业界的主流公司在进行数据的多维度分析的时候，例如说数据仓库，或者基本的一些其它的交互式的分析，就是数据分析层面大家基本上都以Parquet的方式存储数据！Parquet本身就是列式存储的，而且这一列和那一列没有任何关系，它还会进行很多非常高效的优化。Spark SQL底层一般都是接Parquet。

SparkSQLLoadSaveOps.scala
```scala
package com.dt.spark.sparkapps.sql

import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkContext, SparkConf}

/**
 * 第61课：Spark SQL数据加载和保存实战
 * Created by Limaoran on 2016/7/3.
 */
object SparkSQLLoadSaveOps {
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("SparkSQLLoadSaveOps").setMaster("local")
    val sc = new SparkContext(conf)
    val sqlContext = new SQLContext(sc)

    val df = sqlContext.read.format("parquet").load("G:\\runtime\\spark-1.6.0\\examples\\src\\main\\resources\\users.parquet")

    df.write.parquet("z:/users.parquet")

    sc.stop()
  }
}
```


##技术越多不代表技术含量越高！

只要你的公司想要重用你，假设你是搞Spark的人他基本上只让你搞Spark。
只有两种类型的人会搞很多东西：

* 第一种情况就是把你作为一个初级普通工程师，让你搞搞这，搞搞那，基本都是一些配置性工作，或者连接性工作，没什么技术含量！你千万不要认为你搞很多技术是很有技术含量的工作。你如果搞很多技术一般都不会有什么技术含量！
* 第二种情况就是你进入了一个小公司，人才比较少，所以让你干很多。当然你水平很高可以做很多工作！
 
但是你稍微进入一个正规顶级的公司， 他如果真的把你作为高级、资深工程师的话，如果说你做Spark，他只让你做Spark！例如说你进入百度、阿里、腾讯之类的，或者说你进入IBM等之类的，他肯定只让你做这个东西，凡是让你做很多东西的往往都不行，要么公司不行，要么你人不行！
你要知道你只做一件事情的时候，对他而言你的价值是最大的！因为他确定你一定能搞定这个领域里面所有的事情！如果你做很多事情的时候，那么接下来你可能要跳楼么。。。

###你要清楚一件事情：你作为大数据从业人员最轻松的方式一定是绝对精通Spark！这一定是你人生最大的捷径！如果你想给别人打工的话一定是最大的捷径！