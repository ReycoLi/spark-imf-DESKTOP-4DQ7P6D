# 第62课：Spark SQL下Parquet使用最佳实践和代码实战

标签： sparkIMF

---

##一：Spark SQL 下的Parquet使用最佳实践

 1. 过去整个业界对大数据的分析的技术栈的Pipeline一般分为以下两种方式：
    * <font color='red'>Data Source->HDFS->MapReduce/Hive/Spark（相当于ETL）->HDFS Parquet->Spark SQL/Impala->Result Service（可以放在DB中，也有可能被通过JDBC/ODBC来作为数据服务使用）</font>
    * Data Source->Real time update data to HBase/DB->Export to Parquet->Spark SQL/Impala->Result Service（可以放在DB中，也有可能被通过JDBC/ODBC来作为数据服务使用）
    
    上述的第二种方式完全可以通过Kafka+Spark Streaming+Spark SQL（内部也强烈建议采用Parquet的方式来存储数据）的方式取代
 2. 期待的方式：
    **Data Source->Kafka->Fakfa->Spark Streaming->Parquet->Spark SQL（ML、GraphX等）->Parquet->其它各种Data Mining等**

##二：Parquet的精要介绍

Parquet是列式存储格式的一种文件类型，列式存储有以下的核心优势：

 1. 可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。
 2. 压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如Run Length Encoding和Delta Encoding）进一步节约存储空间。
 3. 只读取需要的列，支持向量运算，能够获取更好的扫描性能。

##三：代码实战

###Java版本
SparkSQLParquetOps.java
```java
package com.dtspark.sparkapps.sql;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;

import java.util.List;

/**
 * 第62课：Spark SQL下Parquet使用最佳实践和代码实战
 * Created by Limaoran on 2016/7/3.
 */
public class SparkSQLParquetOps {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("SparkSQLParquetOps");
        conf.setMaster("local");
        JavaSparkContext sc = new JavaSparkContext(conf);
        SQLContext sqlContext = new SQLContext(sc);

        DataFrame usersDF = sqlContext.read().parquet("G:\\runtime\\spark-1.6.0\\examples\\src\\main\\resources\\users.parquet");

        //注册临时表以供后续的SQL查询操作
        usersDF.registerTempTable("users");

        //进行数据的多维度分析
        DataFrame result = sqlContext.sql("select name from users");
        //rdd transformation
        JavaRDD<String> resultRDD = result.javaRDD().map(new Function<Row, String>() {
            @Override
            public String call(Row row) throws Exception {
                return "The name is : "+row.getAs("name");
            }
        });

        List<String> list = resultRDD.collect();
        for(String row : list){
            System.out.println(row);
        }

        sc.close();
    }
}
```

###Scala版，Parquet的Spark官方示例

```scala
package com.dt.spark.sparkapps.sql

import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkContext, SparkConf}

/**
 * 第62课：一个关于Parquet的Spark官方示例
 * Created by Limaoran on 2016/7/3.
 */
object SparkSQLParquetOps {
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("SparkSQLParquetOpsScala").setMaster("local")
    val sc = new SparkContext(conf)
    val sqlContext = new SQLContext(sc)

//    sc.makeRDD()
    import sqlContext.implicits._
    val df1 = sc.parallelize(1 to 5).map(i=>(i,i * 2)).toDF("single","double")
    val df2 = sc.makeRDD(6 to 10).map(i=>(i,i*3)).toDF("single","triple")
    df1.write.parquet("z:/test_table/key=1")
    df2.write.parquet("z:/test_table/key=2")

    //Read the partitioned table
    val df = sqlContext.read.parquet("z:/test_table")
    df.printSchema()
    df.show()

    sc.stop()
  }
}
```

##四：一个关于Parquet的Spark官方示例

```scala
// sqlContext from the previous example is used in this example.
// This is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._

// Create a simple DataFrame, stored into a partition directory
val df1 = sc.makeRDD(1 to 5).map(i => (i, i * 2)).toDF("single", "double")
df1.write.parquet("data/test_table/key=1")

// Create another DataFrame in a new partition directory,
// adding a new column and dropping an existing column
val df2 = sc.makeRDD(6 to 10).map(i => (i, i * 3)).toDF("single", "triple")
df2.write.parquet("data/test_table/key=2")

// Read the partitioned table
val df3 = sqlContext.read.option("mergeSchema", "true").parquet("data/test_table")
df3.printSchema()

// The final schema consists of all 3 columns in the Parquet files together
// with the partitioning column appeared in the partition directory paths.
// root
// |-- single: int (nullable = true)
// |-- double: int (nullable = true)
// |-- triple: int (nullable = true)
// |-- key : int (nullable = true)
```

由此可以延伸出这个非常重要，举个简单的例子：
加入说你做报表的话，每天有报表，每天你又可能分成每个小时的报表，就是小时的报表构成天的报表，天的报表构成周的报表，周的报表构成月的报表，月的报表构成季度的报表，季度的报表构成半年的报表，半年的报表构成一年的报表。
按照时间非常好的组织你的数据，组织好你的数据的话，因为你每次组织的最小单位是一个小时，那你想看一个季度的时候，读取到更上一级的目录就行了！

##问题：
* Parquet到底是怎么分片的？
* Parquet的时候，我们对它的Block大小是怎么控制的？
* Parquet序列胡、反序列胡优化是怎么做的？
* Parquet的pushDown是怎么实现的？


##Spark2.0 DataSet比DataFrame内存空间节约了80%
