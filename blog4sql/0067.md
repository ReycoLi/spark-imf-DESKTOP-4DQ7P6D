# 第67课：Spark SQL下采用Java和Scala实现Join的案例综合实战（巩固前面学习的Spark SQL知识）

标签： sparkIMF

---

##代码实战

###Java版本

SparkSQLwithJoin.java
```java
package com.dtspark.sparkapps.sql;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.sql.*;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;
import scala.Tuple2;

import java.util.ArrayList;
import java.util.List;

/**
 * 第67课：Spark SQL下采用Java实现Join的案例综合实战（巩固前面学习的Spark SQL知识）
 * Created by Limaoran on 2016/7/2.
 */
public class SparkSQLwithJoin {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("SparkSQLwithJoin");
        conf.setMaster("local");
        JavaSparkContext sc = new JavaSparkContext(conf);
        SQLContext sqlContext = new SQLContext(sc);

        //针对json文件数据源来创建DataFrame
        DataFrame peoplesDF = sqlContext.read().json("G:\\runtime\\spark-1.6.0\\examples\\src\\main\\resources\\peoples.json");

        //基于JSON构建的DataFrame来注册临时表
        peoplesDF.registerTempTable("peopleScores");

        //查询出分数大于90的人
        DataFrame execellentScoresDF = sqlContext.sql("select name,score from peopleScores where score>90");

        /**
         * 在DataFrame的基础上转化成为RDD，通过map操作计算出分数大于90的所有人姓名
         */
        List<String> execellentScoresNameList = execellentScoresDF.javaRDD().map(new Function<Row, String>() {
            @Override
            public String call(Row row) throws Exception {
                return row.getAs("name");
            }
        }).collect();

        //动态组拼出JSON
        List<String> peopleInfomations = new ArrayList<String>();
        peopleInfomations.add("{\"name\":\"Michael\",\"age\":20}");
        peopleInfomations.add("{\"name\":\"Andy\",\"age\":17}");
        peopleInfomations.add("{\"name\":\"Justin\",\"age\":19}");

        //通过内容为JSON的RDD来构造DataFrame
        JavaRDD<String> peopleInfomationsRDD = sc.parallelize(peopleInfomations);
        DataFrame  peopleInfomationDF = sqlContext.read().json(peopleInfomationsRDD);

        //注册成为临时表
        peopleInfomationDF.registerTempTable("peopleInfomations");

        String sqlText = "select name ,age from peopleInfomations where name in (";
        for (int i=0;i<execellentScoresNameList.size();i++){
            sqlText+="'"+execellentScoresNameList.get(i)+"',";
        }
        sqlText = sqlText.substring(0,sqlText.length()-1)+")";

        DataFrame execellentNameAgeDF = sqlContext.sql(sqlText);

        JavaPairRDD<String,Tuple2<Integer,Integer>> resultRDD = execellentScoresDF.javaRDD().mapToPair(new PairFunction<Row, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(Row row) throws Exception {
                return new Tuple2<String, Integer>(row.getAs("name"),Integer.valueOf(row.getAs("score") + ""));
            }
        }).join(execellentNameAgeDF.javaRDD().mapToPair(new PairFunction<Row, String, Integer>() {
            @Override
            public Tuple2<String, Integer> call(Row row) throws Exception {
                return new Tuple2(row.getAs("name"), Integer.valueOf(row.getAs("age")+""));
            }
        }));

        JavaRDD<Row> resultRow = resultRDD.map(new Function<Tuple2<String, Tuple2<Integer, Integer>>, Row>() {
            @Override
            public Row call(Tuple2<String, Tuple2<Integer, Integer>> tuple) throws Exception {
                return RowFactory.create(tuple._1(), tuple._2()._2(), tuple._2()._1());
            }
        });

        List<StructField> structFileds = new ArrayList<>();
        structFileds.add(DataTypes.createStructField("name",DataTypes.StringType,false));
        structFileds.add(DataTypes.createStructField("age", DataTypes.IntegerType, true));
        structFileds.add(DataTypes.createStructField("score", DataTypes.IntegerType, true));
        StructType structType = DataTypes.createStructType(structFileds);

        DataFrame personsDF = sqlContext.createDataFrame(resultRow,structType);

        personsDF.write().mode(SaveMode.Overwrite).json("z:/testJson");
        personsDF.write().mode(SaveMode.Overwrite).parquet("z:/testParquet");

        personsDF.show();

        sc.close();
    }
}

```

###Scala版本

SparkSQLwithJoin.scala
```scala
package com.dt.spark.sparkapps.sql

import org.apache.spark.sql.{Row, SaveMode, RowFactory, SQLContext}
import org.apache.spark.sql.types.{StructField, DataTypes}
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ArrayBuffer

/**
 * 第67课：Spark SQL下采用Scala实现Join的案例综合实战（巩固前面学习的Spark SQL知识）
 * Created by Limaoran on 2016/7/3.
 */
object SparkSQLwithJoin {

  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("SparkSQLParquetOpsScala").setMaster("local")
    val sc = new SparkContext(conf)
    val sqlContext = new SQLContext(sc)

    val peopleDF = sqlContext.read.json("G:\\runtime\\spark-1.6.0\\examples\\src\\main\\resources\\peoples.json")

    peopleDF.registerTempTable("peopleScore")

    //查询出分数大于90的人
    val execellentScoresDF = sqlContext.sql("select * from peopleScore where score>90")
    //在DataFrame的基础上转化成为RDD，通过map操作计算出分数大于90的所有人姓名
    val execellentScoresNameList = execellentScoresDF.rdd.map(row=>{  row(0).toString}).collect()

    //动态组拼出JSON
    val peopleInfomations = new ArrayBuffer[String](3)
    peopleInfomations+="{\"name\":\"Michael\",\"age\":20}"
    peopleInfomations+="{\"name\":\"Andy\",\"age\":17}"
    peopleInfomations+="{\"name\":\"Justin\",\"age\":19}"


    //通过内容为JSON的RDD来构造DataFrame
    val peopleInfomationsRDD = sc.parallelize(peopleInfomations)
    val peopleInfomationDF = sqlContext.read.json(peopleInfomationsRDD)

    //注册成为临时表
    peopleInfomationDF.registerTempTable("peopleInfomations")

    val sqlText = new StringBuilder("select name ,age from peopleInfomations where name in (");
    execellentScoresNameList.foreach(name=> sqlText.append("'"+name+"',") )
    sqlText.deleteCharAt(sqlText.length-1)
    sqlText.append(")")

    val execellentNameAgeDF = sqlContext.sql(sqlText.toString())

    //join操作，并且转成<Name,Tuple<Age,Score>>格式
//    peopleScoresDF.join(peopleNameAgeDF).show()
    val resultRDD = execellentScoresDF.rdd.map(row=> Tuple2(row.getString(row.fieldIndex("name")),row.getLong(row.fieldIndex("score")).toInt))
      .join(execellentNameAgeDF.rdd.map(row=>Tuple2(row.getString(row.fieldIndex("name")),row.getLong(row.fieldIndex("age")).toInt)))

    val resultRow = resultRDD.map(tuple=>{Row(tuple._1, tuple._2._2,tuple._2._1)})

    var structFields = new ArrayBuffer[StructField](3)
    structFields+=DataTypes.createStructField("name",DataTypes.StringType,false)
    structFields+=DataTypes.createStructField("age",DataTypes.IntegerType,true)
    structFields+=DataTypes.createStructField("score",DataTypes.IntegerType,true)

    val personsDF = sqlContext.createDataFrame(resultRow,DataTypes.createStructType(structFields.toArray))
    personsDF.write.mode(SaveMode.Overwrite).json("z:/testJson")
    personsDF.write.mode(SaveMode.Overwrite).parquet("z:/testParquet")

    personsDF.show

    sc.stop()
  }
}
```


###测试源数据：

peoples.json
```json
{"name":"Michael","score":98}
{"name":"Andy", "score":95}
{"name":"Justin", "score":58}
```