# 第60课：使用Java和Scala在IDE中实战RDD和DataFrame动态转换操作

标签： sparkIMF

---

如果有人问你Spark是什么？
你就可以告诉他：
**Spark是大数据分析的OS，它首先是一个大数据分析的平台，然后它上面有很多不同的分支，而且我们可以扩展分支，就相当于你可以认为它上面有很多子框架，那开发者基于它的很多子框架可以开发不同类型的程序，例如说机器学习程序、图计算程序、数据的多维度分析程序等、流处理程序。**


##代码实战

案例中Person类和上一讲的内容一样！

###Java版本

RDD2DataFrameByProgrammatically.java
```java
package com.dtspark.sparkapps.sql;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import java.util.ArrayList;
import java.util.List;

/**
 * 第60课：使用动态方式将RDD转换成为DataFrame
 * Created by Limaoran on 2016/7/2.
 */
public class RDD2DataFrameByProgrammatically {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("RDD2DataFrameByProgrammatically");
        conf.setMaster("local");
        JavaSparkContext sc = new JavaSparkContext(conf);
        SQLContext sqlContext = new SQLContext(sc);

        JavaRDD<String> lines = sc.textFile("G:\\txt\\test\\persons.txt");

        /**
         * 第一步：在RDD的基础上创建类型为Row的RDD
         */
        JavaRDD<Row> personsRDD = lines.map(new Function<String, Row>() {
            @Override
            public Row call(String line) throws Exception {
                String [] splited = line.split(",");
                return RowFactory.create(Integer.valueOf(splited[0]),splited[1],Integer.valueOf(splited[2]));
            }
        });
        /**
         * 第二步：动态构造DataFrame的元数据，一般而言，有多少列以及每列的具体类型来自于JSON文件，也可能来自DB
         */
        List<StructField> structFields = new ArrayList<StructField>();
        structFields.add(0, DataTypes.createStructField("id", DataTypes.IntegerType,true));
        structFields.add(1, DataTypes.createStructField("name", DataTypes.StringType,true));
        structFields.add(2, DataTypes.createStructField("age", DataTypes.IntegerType, true));
        //构建StructType，用于最后DataFrame元数据的描述
        StructType structType = DataTypes.createStructType(structFields);

        /**
         * 第三步：基于已有的MetaData以及RDD<Row>来构造DataFrame
         */
        DataFrame personsDF = sqlContext.createDataFrame(personsRDD,structType);

        /**
         * 第四步：注册成为临时表以供后续的SQL查询操作
         */
        personsDF.registerTempTable("persons");

        /**
         * 第五步：进行数据的多维度分析
         */
        DataFrame result = sqlContext.sql("select * from persons where age > 8");

        /**
         * 第六步：对结果进行处理，包括由DataFrame转换成为RDD<Row>，以及结果的持久化
         */
        List<Row> listRow = result.javaRDD().collect();

        for(Row row : listRow){
            System.out.println(row);
        }

        sc.close();
    }
}
```

###Scala版本

RDD2DataFrameByProgrammatically.scala

```scala
package com.dt.spark.sparkapps.sql

import java.util

import org.apache.spark.sql.types.{DataTypes, StructField}
import org.apache.spark.sql.{RowFactory, SQLContext}
import org.apache.spark.{SparkConf, SparkContext}

/**
 * 第60课：使用动态方式将RDD转换成为DataFrame
 * Created by Limaoran on 2016/7/2.
 */
object RDD2DataFrameByProgrammatically {
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("RDD2DataFrameByProgrammaticallyScala").setMaster("local")
    val sc = new SparkContext(conf)
    val sqlContext = new SQLContext(sc)
    val lines = sc.textFile("G:\\txt\\test\\persons.txt")

    //第一步：在RDD的基础上创建类型为Row的RDD
    val personRDD = lines.map(line=>{
      val splited = line.split(",")
      RowFactory.create(Integer.valueOf(splited(0)),splited(1),Integer.valueOf(splited(2)))
    })
    //第二步：动态构造DataFrame的元数据，一般而言，有多少列以及每列的具体类型来自于JSON文件，也可能来自DB
    val structFields = new util.ArrayList[StructField]()
    structFields.add(DataTypes.createStructField("id",DataTypes.IntegerType,true))
    structFields.add(DataTypes.createStructField("name",DataTypes.StringType,true))
    structFields.add(DataTypes.createStructField("age",DataTypes.IntegerType,true))
    //构建StructType，用于最后DataFrame元数据的描述
    val structType = DataTypes.createStructType(structFields)

    //第三步：基于已有的MetaData以及RDD<Row>来构造DataFrame
    val personDF = sqlContext.createDataFrame(personRDD,structType)
    //第四步：注册成为临时表以供后续的SQL查询操作
    personDF.registerTempTable("persons")
    //第五步：进行数据的多维度分析
    val result = sqlContext.sql("select * from persons where age>8")
    //第六步：对结果进行处理，包括由DataFrame转换成为RDD<Row>，以及结果的持久化
    result.collect().foreach(println)

    sc.stop()
  }
}
```

###结果
```text
[2,Hadoop,11]
```

##Spark为什么比Hadoop快？

当存在join、group、其他聚合操作Aggregate或者说其他排序操作的时候，Spark性能优势就极大的体现出来，为什么呢？
因为Hadoop和Spark都有链式操作，但是Hadoop和Spark的链式操作有个本能性的不同，本能性最重要的不同是：Hadoop每个要结果只能有一个Reducer，进行链式操作也只能有一个Reducer，所以就很容易导致一个结果就是一个SQL有Join、Group等一大堆东西就会产生很多作业！而Spark SQL可能就是一个作业！Spark 中Reducer既可以是Mapper，Mapper也可以是Reducer，但是Hadoop不可以！Hadoop每一个Task是一个JVM，而Spark是线程复用！JVM不可以复用！
Hive是解释型的，Spark是CG（Code Generation），代码生成型的！