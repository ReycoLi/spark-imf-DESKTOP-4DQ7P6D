# 第77课：Spark SQL基于网站Log的综合案例实战之Hive更大规模数据导入、Spark SQL对数据UV操作实战

标签： sparkIMF

---

* UV：一个页面被多少用户访问过，一个用户算一个UA，如果一个页面一个用户多次访问也算一个UA

##代码实战

SparkSQLUserLogsOps.java

```java
package com.dtspark.sparkapps.sql;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.SaveMode;
import org.apache.spark.sql.hive.HiveContext;

/**
 * 第76-77课：Spark SQL实战用户日志的输入导入Hive及SQL计算PV、UV实战
 * 执行脚本：spark-submit --class com.dtspark.sparkapps.sql.SparkSQLUserLogsOps --master local /out/sparkApp.jar
 * Created by Limaoran on 2016/7/2.
 */
public class SparkSQLUserLogsOps {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("SparkSQLwithJoin");
        conf.setMaster("local");
        JavaSparkContext sc = new JavaSparkContext(conf);
        HiveContext sqlContext = new HiveContext(sc);
        sqlContext.sql("use hive");

        String date = "2016-07-06";

        pvStatisic(sqlContext,date);
        uvStatisic(sqlContext,date);
        sc.stop();
    }

    /**
     * 第76课：Spark SQL实战用户日志的输入导入Hive及SQL计算PV实战
     * @param hiveContext
     * @param date
     */
    static void pvStatisic(HiveContext hiveContext,String date){
        String sql = "SELECT logdate,pageID,count(*) pv "+
                "FROM userLogs " +
                "WHERE action='View' and logdate='"+date+"' "+
                "GROUP BY logdate,pageID "+
                "ORDER BY pv DESC";

        DataFrame logDF = hiveContext.sql(sql).repartition(1);
        logDF.show();
        //把执行结果放在数据库或者Hive中
        logDF.write().mode(SaveMode.Overwrite).json("/result/pv");
    }

    /**
     * 第77课：Spark SQL对数据UV操作实战
     * @param hiveContext
     * @param date
     */
    static void uvStatisic(HiveContext hiveContext,String date){
        //完整sql示例：SELECT logdate,pageID,count(userID) uv FROM (select distinct logdate,pageID,userID FROM userLogs where action='View' and logdate='2016-07-06') ul GROUP BY logdate,pageID order by uv desc
        String sql = "SELECT logdate,pageID,count(userID) uv "+
                "FROM (select distinct logdate,pageID,userID FROM userLogs where action='View' and logdate='"+date+"') ul " +
                "GROUP BY logdate,pageID "+
                "ORDER BY uv DESC";

        DataFrame logDF = hiveContext.sql(sql).repartition(1);
        logDF.show();
        //把执行结果放在数据库或者Hive中
        logDF.write().mode(SaveMode.Overwrite).json("/result/uv");
    }
}
```

##执行结果

```sh
+----------+------+---+
|   logdate|pageID| uv|
+----------+------+---+
|2016-07-06|   714|  4|
|2016-07-06|  1779|  4|
|2016-07-06|  9002|  3|
|2016-07-06|  3358|  3|
|2016-07-06|  5000|  3|
|2016-07-06|  9731|  3|
|2016-07-06|   378|  3|
|2016-07-06|  4179|  3|
|2016-07-06|  8190|  3|
|2016-07-06|  5708|  3|
|2016-07-06|  2155|  3|
|2016-07-06|  2172|  3|
|2016-07-06|  6977|  3|
|2016-07-06|  4914|  3|
|2016-07-06|  4984|  3|
|2016-07-06|  8584|  3|
|2016-07-06|  2128|  3|
|2016-07-06|  1736|  3|
|2016-07-06|  6745|  3|
|2016-07-06|  1564|  3|
+----------+------+---+
```
