# 第79课：Spark SQL基于网站Log的综合案例综合代码和实际运行测试

标签： sparkIMF

---

##创建表

* use hive;
* create table userlogs2(logdate string,time bigint,userID bigint,pageID bigint,channel string,action string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n';
* load data local inpath 'z:/datanull.txt' into table userlogs2;

##77课-79课完整代码 实战

SparkSQLUserLogsOps.java

```java
package com.dtspark.sparkapps.sql;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.SaveMode;
import org.apache.spark.sql.hive.HiveContext;

/**
 * 第76-79课：Spark SQL实战用户日志的输入导入Hive及SQL计算PV、UV实战
 * 执行脚本：spark-submit --class com.dtspark.sparkapps.sql.SparkSQLUserLogsOps --master local /out/sparkApp.jar
 * Table in hive database creation
 * create table userlogs(logdate string,time bigint,userID bigint,pageID bigint,channel string,action string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n';
 * Created by Limaoran on 2016/7/2.
 */
public class SparkSQLUserLogsOps {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("SparkSQLwithJoin");
        conf.setMaster("local");
//        conf.setMaster("spark://localhost:7077");

        JavaSparkContext sc = new JavaSparkContext(conf);
        HiveContext sqlContext = new HiveContext(sc);
        sqlContext.sql("use hive");

        String date = "2016-07-06";

//        pvStatisic(sqlContext,date);  //PV
//        uvStatisic(sqlContext, date); //UV

        jumpOutStatistic(sqlContext, date);  //页面跳出率
        newUserRegisterPercentStatistic(sqlContext, date); //新用户注册比率
//        hostChannel(sqlContext,date);   //热门板块

        sc.stop();
    }

    /**
     * 第76课：Spark SQL实战用户日志的输入导入Hive及SQL计算PV实战
     * @param hiveContext
     * @param date
     */
    static void pvStatisic(HiveContext hiveContext,String date){
        String sql = "SELECT logdate,pageID,count(*) pv "+
                "FROM userLogs " +
                "WHERE action='View' and logdate='"+date+"' "+
                "GROUP BY logdate,pageID "+
                "ORDER BY pv DESC";

        DataFrame logDF = hiveContext.sql(sql).repartition(1);
        logDF.show();
        //把执行结果放在数据库或者Hive中
        logDF.write().mode(SaveMode.Overwrite).json("/result/pv");
    }

    /**
     * 第77课：Spark SQL对数据UV操作实战
     * @param hiveContext
     * @param date
     */
    static void uvStatisic(HiveContext hiveContext,String date){
        //完整sql示例：SELECT logdate,pageID,count(userID) uv FROM (select distinct logdate,pageID,userID FROM userLogs where action='View' and logdate='2016-07-06') ul GROUP BY logdate,pageID order by uv desc
        String sql = "SELECT logdate,pageID,count(userID) uv "+
                "FROM (select distinct logdate,pageID,userID FROM userLogs where action='View' and logdate='"+date+"') ul " +
                "GROUP BY logdate,pageID "+
                "ORDER BY uv DESC";

        DataFrame logDF = hiveContext.sql(sql).repartition(1);
        logDF.show();
        //把执行结果放在数据库或者Hive中
        logDF.write().mode(SaveMode.Overwrite).json("/result/uv");
    }

    /**
     * 第78课：Spark SQL基于网站Log的综合案例用户用户跳出率
     * @param sqlContext
     * @param date
     */
    private static void jumpOutStatistic(HiveContext sqlContext, String date) {
        //查询这一天只浏览一次页面的用户
//        sqlContext.sql("select logdate,userID,count(*) totalNumber from userlogs where action='View' and logdate='"+date+"' group by logdate,userID having totalNumber=1").show();
        //查询这一天的用户只浏览一次页面的数量
        //select count(*) from (select logdate,userID,count(*) totalNumber from userlogs where action='View' and logdate='2016-07-06' group by logdate,userID having totalNumber=1) rt
        long borwserUserCount = sqlContext.sql("select count(*) " +
                "from (select logdate,userID,count(*) totalNumber from userlogs " +
                "where action='View' and logdate='"+date+"' " +
                "group by logdate,userID having totalNumber=1) rt").javaRDD().collect().get(0).getAs(0);
        //获取这一天的用户数量
        long userTotalCount = sqlContext.sql("select count(distinct(userID)) totalNumber from userlogs where action='View' and logdate='"+date+"'").javaRDD().collect().get(0).getAs(0);

        //计算比率
        //BigDecimal.valueOf(pv1.toString.toDouble) / BigDecimal.valueOf(totalTargetPV.toString.toDouble)
        double bit = borwserUserCount/(double)userTotalCount;
        System.out.println("用户跳出率：" + bit);
    }

    /**
     * 第78课：新用户注册比例
     * @param sqlContext
     * @param date
     */
    private static void newUserRegisterPercentStatistic(HiveContext sqlContext, String date) {
        long userNew = sqlContext.sql("SELECT count(1) " +
                "FROM userlogs " +
                "WHERE action='Register' and logdate='"+date+"' ").collect()[0].getAs(0);
        long userTotalCount = sqlContext.sql("select count(distinct(userID)) totalNumber from userlogs where logdate='"+date+"'").collect()[0].getAs(0);

        //计算比率
        //BigDecimal.valueOf(pv1.toString.toDouble) / BigDecimal.valueOf(totalTargetPV.toString.toDouble)
        double bit = userNew/(double)userTotalCount;
        System.out.println("新用户注册比率：" + bit);
    }
    /**
     * 第79课：Spark SQL基于网站Log的综合案例综合代码和实际运行测试
     * @param sqlContext
     * @param date
     */
    private static void hostChannel(HiveContext sqlContext, String date) {
        String sql = "select logdate,channel,count(1) channelpv FROM userlogs " +
                "where action='View' AND logdate='"+date+"' " +
                "GROUP BY logdate,channel " +
                "ORDER BY channelpv DESC";

        sqlContext.sql(sql).show();
    }
}
```
