# 第58课：使用Java和Scala在IDE中开发DataFrame实战

标签： sparkIMF

---

## HiveContext继承SQLContext

一般情况下，如果不操作数据仓库的时候一般都用SQLContext来进行数据的多维度分析，但是**Spark官方文档建议任何时候都用 HiveContext！**理由：虽然说HiveContext可以针对Hive进行操作，但是它也具备了SQLContext的所有功能，而且功能更加丰富！

你可以用HiveContext进行各种操作，没有必要再去使用SQLContext。

##DataFrame案例实战

###Java版代码：
```java
package com.dtspark.sparkapps.sql;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.SQLContext;

/**
 * 使用Java的方式开发实战对DataFrame的操作
 * Created by Limaoran on 2016/7/2.
 */
public class DataFrameOps {
    public static void main(String[] args) {
        //创建SparkConf用于读取系统配置信息并设置当前应用程序的名称
        SparkConf conf = new SparkConf();
        conf.setMaster("spark://master:7077");
        conf.setAppName("DataFrameOps");
        //创建JavaSparkContext对象实例作为整个Driver的核心基石
        JavaSparkContext sc = new JavaSparkContext(conf);
        //创建SQLContext上下文对象用于SQL的分析
        SQLContext sqlContext = new SQLContext(sc);

        //创建DataFrame，可以简单的认为DataFrame是一张表
        DataFrame df = sqlContext.read().json("hdfs://Master:9000/library/examples/src/main/resources/people.json");

        //select * from table
        df.show();

        //desc table
        df.printSchema();

        //select name from table
        df.select("name").show();

        //select name,age+10 from table
        df.select(df.col("name"), df.col("age").plus(10)).show();

        //select * from table where age>10
        df.filter(df.col("age").gt(10)).show();

        //select age,count(1) from table group by age
        df.groupBy(df.col("age")).count().show();

        sc.close();
    }
}
```

###Scala版代码：
```scala
package com.dt.spark.sparkapps.sql

import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkContext, SparkConf}

/**
 * 使用Scala开发DataFrame案例
 * Created by Limaoran on 2016/7/2.
 */
class DataFrameOps {
  def main(args: Array[String]) {
    val conf = new SparkConf()
    conf.setAppName("DataFrameOpsScala")
//    conf.setMaster("local")
    val sc = new SparkContext(conf)
    val sqlContext = new SQLContext(sc)
    val df = sqlContext.read.json("hdfs://Master:9000/library/examples/src/main/resources/people.json")
    df.show()
    df.printSchema
    df.select("name").show
    df.select(df("name"),df("age")+10).show()
    df.filter(df("age")>10).show()
    df.groupBy(df("age")).count().show()

  }
}
```

###将程序打包为jar文件

###编写提交spark程序脚本：runSparkApp.sh
```sh
$SPARK_HOME/bin/spark-submit --class com.dt.spark.sparkapps.sql.DataFrameOps --master spark://Master:7077 /home/sparkApp/sparkApp.jar
```


###在生产环境下推荐使用：

```sh
$SPARK_HOME/bin/spark-submit --class com.dt.spark.sparkapps.sql.DataFrameOps --master spark://Master:7077 /home/sparkApp/sparkApp.jar --files /usr/local/hive-1.2.1/conf/hive-site.xml --driver-class-path /usr/local/hive-1.2.1/lib/mysql-connector-java-5.1.13-bin.jar
```

执行runSparkApp.sh
```sh
# ./runSparkApp.sh
```
