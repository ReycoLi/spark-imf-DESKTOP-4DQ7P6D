# 第97课：使用Spark Streaming+Spark SQL实现在线动态计算出特定时间窗口下的不同种类商品中的热门商品排名

标签： sparkIMF

---

##代码实战

OnlineTheTop3ItemForEachCategory2DB.scala
```scala
package com.dt.spark.sparkapps.streaming

import com.dtspark.sparkstreaming.ConnectionPool
import org.apache.spark.SparkConf
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types.{IntegerType, StructField, StringType, StructType}
import org.apache.spark.streaming.{Seconds, StreamingContext}


/**
 * 第97课：使用Spark Streaming+Spark SQL实现在线动态计算出特定时间窗口下的不同种类商品中的热门商品排名
 * Created by Limaoran on 2016/5/4.
 *
 * 使用Spark Streaming+Spark SQL 来在线动态计算电商中不同类别中最热门的商品排名，
 * 例如手机这个类别下面最热门的三种手机，电视这个类别下最热门的三种电视，该实例在实际生产环境下具有重大的意义；
 *
 * 背景描述：
 * 实现技术：Spark Streaming+Spark SQL，之所以Spark Streaming能够使用ML、sql、graghx等功能
 *  是因为有foreachRDD和Transform等接口，这些接口中其实是基于RDD进行操作，
 *  所以以RDD为基石就可以直接使用其他Spark所有的功能，就像直接调用API一样简单。
 *
 *  假设说这里的数据的格式：user item category，例如Rocky Samsung Android
 *
 * 执行脚本：spark-submit --class com.dt.spark.sparkapps.streaming.OnlineTheTop3ItemForEachCategory2DB --master local /out/sparkApp.jar --driver-class-path E:/OneDrive/lib/mysql-connector-java-5.1.13-bin.jar --files G:/runtime/spark-1.6.0/conf/hive-site.xml
 */
object OnlineTheTop3ItemForEachCategory2DB {
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("OnlineTheTop3ItemForEachCategory2DB")  //设置应用程序的名称，在程序运行的监控界面可以看到名称
    conf.setMaster("local[4]") //此时，程序在Spark集群

    val ssc = new StreamingContext(conf,Seconds(10))
//    ssc.sparkContext.setLogLevel("ERROR")
    ssc.checkpoint("z:/checkpoint")
//    ssc.checkpoint("/library/checkpoint")
    //监听主机Master上的9999端口，接收数据
    val userClickLogsDStream = ssc.socketTextStream("Master" ,9999)

    val formattedUserClickLogsDStream = userClickLogsDStream.map(click =>
        (click.split(" ")(2)+"_"+click.split(" ")(1),1))

    //任意一种商品它在过去60秒钟被点击了多少次
//    val categoryUserClickLogsDStream = formattedUserClickLogsDStream.reduceByKeyAndWindow( (v1,v2)=>v1+v2 , (v1,v2)=>v1-v2,Seconds(60),Seconds(20) )
    val categoryUserClickLogsDStream = formattedUserClickLogsDStream.reduceByKeyAndWindow( _+_ , _-_ ,Seconds(60),Seconds(20) )

    categoryUserClickLogsDStream.foreachRDD(rdd=>{
      if(rdd.isEmpty()){
        println("No data inputed!")
      } else{
        val categoryItemRow = rdd.map(reduceItem=> {
          val category = reduceItem._1.split("_")(0)
          val item = reduceItem._1.split("_")(1)
          val click_count = reduceItem._2
          Row(category,item,click_count)
        } )
        val structType = StructType(Array(
          StructField("category",StringType,true),
          StructField("item",StringType,true),
          StructField("click_count",IntegerType,false)
        ))
        val hiveContext = new HiveContext(rdd.context)
        val categoryItemDF = hiveContext.createDataFrame(categoryItemRow,structType)

        categoryItemDF.registerTempTable("categoryItemTable")

        val resultDataFrame = hiveContext.sql("SELECT category,item,click_count from (SELECT category,item,click_count,row_number()" +
          " OVER (PARTITION BY category ORDER BY click_count DESC ) rank FROM categoryItemTable) subquery" +
          " WHERE rank<=3 ")

        resultDataFrame.show()

        val resultRowRDD = resultDataFrame.rdd
        resultRowRDD.foreachPartition(partitionOfRecords=>{
          if(partitionOfRecords.isEmpty){
            println("RDD is not null,but partition is null !!!")
          }else{
            val connection = ConnectionPool.getConnection()
            partitionOfRecords.foreach(record=>{
              val sql = "insert into categorytop3(category,item,click_count) values('"+
                record.getAs("category")+"','"+record.getAs("item")+"',"+record.getAs("click_count")+")";
              val stmt = connection.createStatement()
              stmt.execute(sql)
            })
            println("sql executor over!")
            ConnectionPool.returnConnection(connection)
          }
        })
      }
    })

    ssc.start()
    ssc.awaitTermination()
  }
}
```


##部署脚本
```sh
spark-submit --class com.dt.spark.sparkapps.streaming.OnlineTheTop3ItemForEachCategory2DB --master local /out/sparkApp.jar --driver-class-path E:/OneDrive/lib/mysql-connector-java-5.1.13-bin.jar --files G:/runtime/spark-1.6.0/conf/hive-site.xml
```

##测试前先创建MySQL表

```sql
create table categorytop3(category varchar(128),item varchar(128),`count` int );
```

##测试数据


```sh
# nc -lk 9999
peter samsung androidphone
mike huawei androidphone
jim xiaomi androidphone
jaker apple applephone
lili samsung androidphone
```

##测试结果

|category|item|click_count|
|--|--|--|
|androidphone|samsung|4|
|applephone|apple|2|
|androidphone|huawei|2|
|androidphone|xiaomi|2|
