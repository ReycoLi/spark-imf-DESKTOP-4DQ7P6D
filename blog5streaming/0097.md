# 第97课：使用Spark Streaming+Spark SQL实现在线动态计算出特定时间窗口下的不同种类商品中的热门商品排名

标签： sparkIMF

---


##代码实战

OnlineTheTop3ItemForEachCategory2DB.scala
```scala
package com.dt.spark.sparkapps.streaming

import com.dtspark.sparkstreaming.ConnectionPool
import org.apache.spark.SparkConf
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types.{IntegerType, StructField, StringType, StructType}
import org.apache.spark.streaming.{Seconds, StreamingContext}


/**
 * 第97课：使用Spark Streaming+Spark SQL实现在线动态计算出特定时间窗口下的不同种类商品中的热门商品排名
 * Created by Limaoran on 2016/5/4.
 *
 * 使用Spark Streaming+Spark SQL 来在线动态计算电商中不同类别中最热门的商品排名，
 * 例如手机这个类别下面最热门的三种手机，电视这个类别下最热门的三种电视，该实例在实际生产环境下具有重大的意义；
 *
 * 背景描述：
 * 实现技术：Spark Streaming+Spark SQL，之所以Spark Streaming能够使用ML、sql、graghx等功能
 *  是因为有foreachRDD和Transform等接口，这些接口中其实是基于RDD进行操作，
 *  所以以RDD为基石就可以直接使用其他Spark所有的功能，就像直接调用API一样简单。
 *
 *  假设说这里的数据的格式：user item category，例如Rocky Samsung Android
 *
 * 执行脚本：spark-submit --class com.dt.spark.sparkapps.streaming.OnlineTheTop3ItemForEachCategory2DB --master local /out/sparkApp.jar --driver-class-path E:/OneDrive/lib/mysql-connector-java-5.1.13-bin.jar --files G:/runtime/spark-1.6.0/conf/hive-site.xml
 */
object OnlineTheTop3ItemForEachCategory2DB {
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("OnlineTheTop3ItemForEachCategory2DB")  //设置应用程序的名称，在程序运行的监控界面可以看到名称
    conf.setMaster("local[4]") //此时，程序在Spark集群

    val ssc = new StreamingContext(conf,Seconds(10))
//    ssc.sparkContext.setLogLevel("ERROR")
    ssc.checkpoint("z:/checkpoint")
//    ssc.checkpoint("/library/checkpoint")
    //监听主机Master上的9999端口，接收数据
    val userClickLogsDStream = ssc.socketTextStream("Master" ,9999)

    val formattedUserClickLogsDStream = userClickLogsDStream.map(click =>
        (click.split(" ")(2)+"_"+click.split(" ")(1),1))

    //任意一种商品它在过去60秒钟被点击了多少次
//    val categoryUserClickLogsDStream = formattedUserClickLogsDStream.reduceByKeyAndWindow( (v1,v2)=>v1+v2 , (v1,v2)=>v1-v2,Seconds(60),Seconds(20) )
    val categoryUserClickLogsDStream = formattedUserClickLogsDStream.reduceByKeyAndWindow( _+_ , _-_ ,Seconds(60),Seconds(20) )

    categoryUserClickLogsDStream.foreachRDD(rdd=>{
      if(rdd.isEmpty()){
        println("No data inputed!")
      } else{
        val categoryItemRow = rdd.map(reduceItem=> {
          val category = reduceItem._1.split("_")(0)
          val item = reduceItem._1.split("_")(1)
          val click_count = reduceItem._2
          Row(category,item,click_count)
        } )
        val structType = StructType(Array(
          StructField("category",StringType,true),
          StructField("item",StringType,true),
          StructField("click_count",IntegerType,false)
        ))
        val hiveContext = new HiveContext(rdd.context)
        val categoryItemDF = hiveContext.createDataFrame(categoryItemRow,structType)

        categoryItemDF.registerTempTable("categoryItemTable")

        val resultDataFrame = hiveContext.sql("SELECT category,item,click_count from (SELECT category,item,click_count,row_number()" +
          " OVER (PARTITION BY category ORDER BY click_count DESC ) rank FROM categoryItemTable) subquery" +
          " WHERE rank<=3 ")

        resultDataFrame.show()

        val resultRowRDD = resultDataFrame.rdd
        resultRowRDD.foreachPartition(partitionOfRecords=>{
          if(partitionOfRecords.isEmpty){
            println("RDD is not null,but partition is null !!!")
          }else{
            val connection = ConnectionPool.getConnection()
            partitionOfRecords.foreach(record=>{
              val sql = "insert into categorytop3(category,item,click_count) values('"+
                record.getAs("category")+"','"+record.getAs("item")+"',"+record.getAs("click_count")+")";
              val stmt = connection.createStatement()
              stmt.execute(sql)
            })
            println("sql executor over!")
            ConnectionPool.returnConnection(connection)
          }
        })
      }
    })

    ssc.start()
    ssc.awaitTermination()
  }
}
```

在这里你可能会遇到一个问题，比如RDD为空，但是Partition不为空？
因为：**我们这个处理是reduceByKeyAndWindow，我们这里是5秒钟的时间，每5秒是一个时间窗口，那我能不能确保这个60秒钟的每5秒钟里面都有数据呢？这个不能确保！但是这个地方foreachRDD中的rdd是从reduceByKeyAndWindow的结果去考虑的，这个rdd是很多RDD的聚合的结果，进行Aggregate聚合的结果！它进行Aggregate聚合的时候其实它自己也有一个分片的问题。凡是我们输入数据的话，那这个RDD肯定不为空，因为我们输入了数据，所以它不为空。然后进行else操作的时候由于它聚合了很多RDD，因为它是按照时间片生成的，这个RDD是从reduceByKeyAndWindow去考虑，然后这边在进行计算的时候有很多Partition，这个Partition有的有数据，有的没数据，因为我们是reduceByKeyAndWindow。**正常的RDD里面肯定是有数据的，RDD的每个Partition肯定是有数据的，但是这个就是Key倾斜的问题以及输入数据的问题。因为reduceByKeyAndWindow是把很多RDD聚合成为一个RDD，聚合的时候它有自己的分片规则，分片规则导致的你正好分的那个片的那个时间窗口的RDD没有数据，这种可能性对于我们这种reduceByKeyAndWindow就特别明显了！
但是如果一个普通的RDD这种肯能性就不太大了，除非是进行shuffle操作的时候，你很不走运气，就是你这个Key如此的不平衡，以至于那个Partition就真的一条数据也没有。但那种情况也有可能出现RDD正常，Partition也存在，但是Partition没有数据的情况，那种情况叫做数据倾斜！

##部署脚本
```sh
spark-submit --class com.dt.spark.sparkapps.streaming.OnlineTheTop3ItemForEachCategory2DB --master local /out/sparkApp.jar --driver-class-path E:/OneDrive/lib/mysql-connector-java-5.1.13-bin.jar --files G:/runtime/spark-1.6.0/conf/hive-site.xml
```

##测试前先创建MySQL表

```sql
create table categorytop3(category varchar(128),item varchar(128),`count` int );
```

##测试数据


```sh
# nc -lk 9999
peter samsung androidphone
mike huawei androidphone
jim xiaomi androidphone
jaker apple applephone
lili samsung androidphone
```

##测试结果

|category|item|click_count|
|--|--|--|
|androidphone|samsung|4|
|applephone|apple|2|
|androidphone|huawei|2|
|androidphone|xiaomi|2|
