# 第94课：SparkStreaming 实现广告计费系统中在线黑名单过滤实战

标签： sparkIMF

---

##代码实战

###Scala版

OnlineBlackListFilter.scala
```scala
package com.dt.spark.sparkapps.streaming

import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}


/**
 * 第94课：SparkStreaming 实现广告计费系统中使用Scala开发集群运行的Spark在线黑名单过滤程序
 *
 * Created by Limaoran on 2016/5/2.
 * 新浪微博：http://weibo.com/ilovepains/
 *
 * 背景描述：在广告点击计费系统中，我们在线过滤掉黑名单的点击，进而保护广告商的利益，
 * 只进行有效的广告点击计费或者在防刷评分（或者流量）系统，过滤掉无效的投票或者评分或者流量；
 * 实现技术：使用transform Api直接基于RDD编程，进行join操作
 *
 */
object OnlineBlackListFilter {
  def main(args: Array[String]) {
    /**
     * 第1步：创建Spark的配置对象SparkConf，设置Spark程序的运行时的配置信息。
     * 例如说通过setMaster来设置程序要链接的Spark集群的Master的URL,如果设置
     * 为local，则代表Spark程序在本地运行，特别适合于机器配置条件非常差（例如
     * 只有1G的内存）的初学者
     */
    val conf = new SparkConf()  //创建SparkConf对象
    conf.setAppName("OnlineBlackListFilter")  //设置应用程序的名称，在程序运行的监控界面可以看到名称
//    conf.setMaster("spark://Master:7077") //此时，程序在Spark集群
//    conf.setMaster("spark://localhost:7077")
    conf.setMaster("local[2]") //此时，程序在Spark集群

    val ssc = new StreamingContext(conf,Seconds(30))

    /**
     * 黑名单数据准备，实际上黑名单一般都是动态的，例如在Redis或者数据库中，黑名单的生成往往有复杂的业务
     * 逻辑，具体情况算法不同，但是在Spark Streaming进行处理的时候每次都能工访问完整的信息
     */
    val blackList = Array(("hadoop",true),("mahout",true))
    val blackListRDD = ssc.sparkContext.parallelize(blackList,8)

    //监听主机Master上的9999端口，接收数据
    val adsClickStream = ssc.socketTextStream("Master" ,9999)
    /**
     * 此处模拟的广告点击的每条数据的格式为：time、name
     * 此处map操作的结果是name、（time，name）的格式
     */
    val adsClientStreamFormated = adsClickStream.map(ads=>(ads.split(" ")(1),ads))
    adsClientStreamFormated.transform(userClickRDD => {
      //通过leftOuterJoin操作既保留了左侧用户广告点击内容的RDD的所有内容，又获得了相应点击内容是否在黑名单中
      val joinedBlackListRDD = userClickRDD.leftOuterJoin(blackListRDD)
      /**
       * 进行filter过滤的时候，其输入元素是一个Tuple：（name,((time,name), boolean)）
       * 其中第一个元素是黑名单的名称，第二元素的第二个元素是进行leftOuterJoin的时候是否存在在值
       * 如果存在的话，表面当前广告点击是黑名单，需要过滤掉，否则的话则是有效点击内容；
       */
      val validClicked = joinedBlackListRDD.filter(joinedItem=>{
        if(joinedItem._2._2.getOrElse(false)){
          false
        }else{
          true
        }
      })
      validClicked.map(validClick => {validClick._2._1})
    }).print()
    /**
     * 计算后的有效数据一般都会写入Kafka中，下游的计费系统会从kafka中pull到有效数据进行计费
     */
    ssc.start()
    ssc.awaitTermination()
  }
}
```

###Java版

OnlineBlackListFilter.java
```java
package com.dtspark.sparkapps.streaming;

import com.google.common.base.Optional;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.Time;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import scala.Tuple2;

import java.util.ArrayList;
import java.util.List;

/**
 * 第94课：SparkStreaming 实现广告计费系统中使用Java开发集群运行的Spark在线黑名单过滤程序
 *
 * 背景描述：在广告点击计费系统中，我们在线过滤掉黑名单的点击，进而保护广告商的利益，
 *  只进行有效的广告点击计费，或者在防刷评分（或者流量）系统，过滤掉无效的投票或者评分或者流量；
 * 实现技术：使用transform API直接基于RDD编程，进行join操作
 * Created by Limaoran on 2016/7/12.
 */
public class OnlineBlackListFilter {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("OnlineBlackListFilter4Ja").setMaster("local[4]");
        JavaStreamingContext jsc = new JavaStreamingContext(conf, Durations.seconds(30));
        /**
         * 黑名单数据准备，实际上黑名单一般都是动态的，例如说Redis或者数据库中，
         * 黑名单的生成往往有复杂的业务逻辑，具体的情况算法不同，但是在Spark Streaming进行处理的时候
         * 每次都能够访问完整的信息。
         */
        List blackList = new ArrayList<>();
        blackList.add(new Tuple2("hadoop", true));
        blackList.add(new Tuple2("mahout", true));
        blackList.add(new Tuple2("flink", true));

        JavaPairRDD<String,Boolean> blackListRDD = jsc.sparkContext().parallelizePairs(blackList,1);

        /**
         * 此处模拟的广告点击的每条数据的格式为：time name
         */
        JavaReceiverInputDStream<String> adsClickStream = jsc.socketTextStream("Master", 9999);
        //此处map操作的结果是name、（time，name）的格式
        JavaPairDStream<String,String>  adsClickStreamFormat  = adsClickStream.mapToPair(ads -> {
            return new Tuple2(ads.split(" ")[1], ads);
        });

        JavaDStream<String> resultDSteam = adsClickStreamFormat.transform(new Function<JavaPairRDD<String, String>, JavaRDD<String>>() {
            @Override
            public JavaRDD<String> call(JavaPairRDD<String, String> userClickRDD) throws Exception {
                //通过leftOuterJoin操作既保留了左侧用户广告点击内容的RDD的所有内容，
                // 又获得了相应点击内容元素
                JavaPairRDD<String,Tuple2<String,Optional<Boolean>>> joinedBlackListRDD  = userClickRDD.leftOuterJoin(blackListRDD);
                /**
                 * 进行filter过滤的时候，其输入元素是一个Tuple:(name,((time,name),boolean)
                 * 其中第一个元素是黑名单的名称，第二个元素的第二个元素是进行leftOutJoin的时候是否存在该值，
                 * 如果存在的话，表明当前广告点击是黑名单，需要过滤掉，否则的话则是有效点击内容！
                 */
                JavaPairRDD<String,Tuple2<String,Optional<Boolean>>> validClicked = joinedBlackListRDD.filter(new Function<Tuple2<String, Tuple2<String, Optional<Boolean>>>, Boolean>() {
                    @Override
                    public Boolean call(Tuple2<String, Tuple2<String, Optional<Boolean>>> joinedItem) throws Exception {
                        if(joinedItem._2()._2().isPresent() && true == joinedItem._2()._2().get()){
                            return false;   //如果黑名单存在，而且是true，则过滤掉！ 这里filter函数式把false给留下了，true返回
                        }else{
                            return true;
                        }
                    }
                });
                JavaRDD<String> result = validClicked.map(item->item._2()._1());

                return result;
            }
        });
        resultDSteam.print();

        jsc.start();
        jsc.awaitTermination();
    }
}
```

##测试数据

linux发送测试数据  nc -lk 9999

```sh
# nc -lk 9999
22555 spark
244321 hadoop
6666 HDFS
2222 Kafka
66666 Java
66662 mahout
```

##过滤结果：

```text
-------------------------------------------
Time: 1468308480000 ms
-------------------------------------------
2222 Kafka
66666 Java
22555 spark
6666 HDFS
```
