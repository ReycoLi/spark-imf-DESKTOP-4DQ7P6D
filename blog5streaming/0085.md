# 第85课：基于HDFS的SparkStreaming案例实战和内幕源码解密

标签： sparkIMF

---

创建HDFS目录

* hadoop dfs -mkdir /library/sparkstreaming
* hadoop dfs -mkdir /library/sparkstreaming/checkpointData


##代码实战

SparkStreamingOnHDFS.java

```java
package com.dtspark.sparkapps.streaming;

import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.api.java.JavaStreamingContextFactory;
import scala.Tuple2;
import scala.actors.threadpool.Arrays;

/**
 * 第85课：基于HDFS的SparkStreaming案例实战
 * Created by Limaoran on 2016/7/10.
 */
public class SparkStreamingOnHDFS {
    public static void main(String[] args) {
        String hdfs = "hdfs://MasterWin:9000";
        hdfs = "z:";
        String pathData = hdfs + "/library/sparkstreaming";
        String pathCheckpoint = hdfs + "/library/checkpoint/";
        final String checkpointDirectory = pathCheckpoint ;
        JavaStreamingContextFactory factory = new JavaStreamingContextFactory(){
            @Override
            public JavaStreamingContext create() {
                return createContext(checkpointDirectory,"SparkStreamingOnHDFS");
            }
        };
        /**
         * 可以从失败中恢复Driver，不过还需要指定Driver这个进程运行在Cluster，
         *  并且在提交应用程序的时候指定--supervise;
         */
        JavaStreamingContext jsc = JavaStreamingContext.getOrCreate(checkpointDirectory,factory);
        /**
         * 此处没有Receiver，SparkStreaming应用程序只是按照时间间隔监控目录下每个Batch新增的内容
         *  （把新增的）作为RDD的数据来源生成原始RDD。
         */
        JavaDStream<String> datadstream = jsc.textFileStream(pathData);  //注意这里监听的文件，检测属性是创建时间
        JavaDStream<String> words = datadstream.flatMap(line -> Arrays.asList(line.split(" ")));
        JavaPairDStream<String,Integer> wordsCount = words.mapToPair(word -> new Tuple2<String, Integer>(word, 1));
        JavaPairDStream<String,Integer> wordsCounts = wordsCount.reduceByKey((v1,v2)->v1+v2);
        wordsCounts.print();
//        wordsCounts.foreachRDD(rdd->{
//            rdd.foreach(tuple -> {
//                System.out.println("Key:"+tuple._1()+",Value:"+tuple._2());
//            });
//        });
        jsc.start();
        jsc.awaitTermination();
    }
    static JavaStreamingContext createContext(String checkpointDirectory,String appName){
        System.out.println("create New Context");
        SparkConf conf = new SparkConf().setAppName(appName).setMaster("local[2]");
        JavaStreamingContext ssc = new JavaStreamingContext(conf, Durations.seconds(15));
        ssc.checkpoint(checkpointDirectory);
        return ssc;
    }
}
```
