# 第88课：SparkStreaming 从Flume Poll数据案例实战和内幕源码解密

标签： sparkIMF

---


##配置好flume-pull

* flume-pull.properties

```properties
#agent2表示代理名称
agent2.sources = source1
agent2.channels = channel1
agent2.sinks = sink1

#配置source1
# For each one of the sources, the type is defined
agent2.sources.source1.type = spooldir
#agent2.sources.source1.spooldir=/usr/local/flume-1.6.0/tmp/TestDir
agent2.sources.source1.spoolDir=z:/flumeTmpDir/TestDir
agent2.sources.source1.ignorePattern = ^(.)*\\.tmp$
# The channel can be defined as follows.
agent2.sources.source1.channels = channel1

agent2.sources.source1.fileHeader = false
agent2.sources.source1.interceptors = i1
agent2.sources.source1.interceptors.i1.type = timestamp

#配置sink1
#agent2.sinks.sink1.type=hdfs
#agent2.sinks.sink1.hdfs.path=hdfs://MasterWin:9000/library/flume
#agent2.sinks.sink1.hdfs.fileType=DataStream
#agent2.sinks.sink1.hdfs.writeFormat=TEXT
#agent2.sinks.sink1.hdfs.rollInterval=1
#agent2.sinks.sink1.hdfs.filePrefix=%Y-%m-%d

#agent2.sinks.sink1.type = avro
#agent2.sinks.sink1.channel = channel1
#agent2.sinks.sink1.hostname = MasterWin
#agent2.sinks.sink1.port = 9999

agent2.sinks = spark
agent2.sinks.spark.type = org.apache.spark.streaming.flume.sink.SparkSink
agent2.sinks.spark.hostname = MasterWin
agent2.sinks.spark.port = 9999
agent2.sinks.spark.channel = channel1
#memoryChannel

#配置channel1
# Each channel's type is defined.
agent2.channels.channel1.type=file
#agent2.channels.channel1.checkpointDir=/usr/local/flume-1.6.0/tmp/checkpointDir
#agent2.channels.channel1.dataDirs=/usr/local/flume-1.6.0/tmp/dataDirs
agent2.channels.channel1.checkpointDir=z:/flumeTmpDir/checkpointDir
agent2.channels.channel1.dataDirs=z:/flumeTmpDir/dataDirs
```

* [下载jar包](http://spark.apache.org/docs/latest/streaming-flume-integration.html)放入flume/lib目录下
    * spark-streaming-flume-sink_2.10-1.6.2.jar
    * scala-library-2.10.5.jar
    * commons-lang3-3.3.2.jar
* 启动Flume脚本
```sh
flume-ng agent -c conf -f conf/flume-pull.properties -n agent2
```
    linux系统下可以参加参数-Dflume.root.logger=INFO,console
    
* 把文件复制到Z:\flumeTmpDir\TestDir观察控制台变化
* 如果出现错误 比如“MalformedInputException: Input length = 1”等，把文件编码改为UTF-8再做测试！



##代码实战

SparkStreamingPullDataFromFlume.java

```java
package com.dtspark.sparkapps.streaming;

import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.flume.FlumeUtils;
import org.apache.spark.streaming.flume.SparkFlumeEvent;
import scala.Tuple2;
import scala.actors.threadpool.Arrays;

/**
 * 第88课：SparkStreaming 从Flume Poll数据案例实战
 * Created by Limaoran on 2016/7/11.
 */
public class SparkStreamingPullDataFromFlume {
    public static void main(String[] args) {
        String pathCheckpoint = "z:/checkpoint/";

        SparkConf conf = new SparkConf().setAppName("FlumePushData2SparkStreaming").setMaster("local[4]");
        JavaStreamingContext jsc = new JavaStreamingContext(conf, Durations.seconds(15));
        jsc.checkpoint(pathCheckpoint);

        JavaReceiverInputDStream<SparkFlumeEvent> receiverInputDStream = FlumeUtils.createPollingStream(jsc,"MasterWin",9999);
        JavaDStream<String> words = receiverInputDStream.flatMap(event ->{
            String line = new String( event.event().getBody().array());
            return Arrays.asList(line.split(" "));
        });
        JavaPairDStream<String,Integer> wordsCount = words.mapToPair(word -> new Tuple2<String, Integer>(word, 1));
        JavaPairDStream<String,Integer> wordsCounts = wordsCount.reduceByKey((v1,v2)->v1+v2);
        wordsCounts.print();
//        wordsCounts.foreachRDD(rdd->{
//            rdd.foreach(tuple -> {
//                System.out.println("Key:"+tuple._1()+",Value:"+tuple._2());
//            });
//        });
        jsc.start();
        jsc.awaitTermination();
    }
}
```

##源码阅读

* FlumeUtils.createPollingStream
* FlumePollingReceiver
* FlumePollingInputDStream
* FlumeBatchFetcher