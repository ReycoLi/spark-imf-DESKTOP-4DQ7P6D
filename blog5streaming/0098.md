# 第98-99课：使用Spark Streaming实战对论坛网站动态行为的多维度分析（上、下）

标签： sparkIMF

---


##自动向Kafka发送数据

SparkStreamingDataManuallyProducer4Kafka.java
```java
package com.dtspark.sparkapps.streaming;

import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;

import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Properties;
import java.util.Random;

/**
 * 第98课：论坛数据自动生成代码，该生成的数据会作为Producer的方式发送给Kafka，
 *  然后SparkStreaming程序会从Kafka中在线pull到论坛或者网站的用户在线行为信息，
 *  进而进行多维度的在线分析
 *
 * 数据格式如下：
 * date：日期，格式为yyyy-MM-dd
 * timestamp：时间戳
 * userID：用户ID
 * pageID：访问的页面ID
 * chanelID：板块ID
 * action：点击、注册和登录
 *
 * 调整运行时的参数：
 *  -verbose:gc -Xmx1G -Xmx1G -Xss128k -XX:+PrintGCDetails
 *
 * Kafka Topic的创建：
 *  kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic UserLogs
 * Kafka Consumer运行：
 *  kafka-console-consumer --zookeeper localhost:2181 --topic UserLogs --from-beginning
 * Created by Limaoran on 2016/7/13.
 */
public class SparkStreamingDataManuallyProducer4Kafka implements Runnable {

    private String dateToDay = null;

//    static SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd");
    //具体的论坛频道
    static String [] channelNames = new String[]{
            "Spark","Scala","Kafka","Flink","Hadoop",
            "Storm","Hive","Impala","HBase","Java"
    };
    //用户的行为模式
    static String [] actionNames = new String[]{
            "View","Register","Login"
    };
    //发送给Kafka的数据的类型
    private String topic;

    Random random = new Random();
    private Producer<Integer,String> producer = null;

    public SparkStreamingDataManuallyProducer4Kafka(String topic){
        dateToDay = new SimpleDateFormat("yyyy-MM-dd").format(new Date());
        this.topic = topic;
        //连接Kafka
        Properties props = new Properties();
        props.put("metadata.broker.list","localhost:9092");
        props.put("serializer.class","kafka.serializer.StringEncoder");
        ProducerConfig config = new ProducerConfig(props);
        producer = new Producer<>(config);
    }
    @Override
    public void run() {
        int counter = 0;
        while(true) {
            String msg = userlogs();
            System.out.println("product:" + msg);
            producer.send(new KeyedMessage<Integer, String>(topic, msg));
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
    }

    private String userlogs() {
        StringBuilder userLogBuffer = new StringBuilder();
        //昨天的时间的生成
        long timestamp = new Date().getTime();
        //随机生成的用户ID
        int userId = random.nextInt(100);
        //随机生成的页面ID
        int pageId = random.nextInt(200);
        //随机生成的channelID
        String channel = channelNames[random.nextInt(channelNames.length)];
        //随机生成actioin行为
        String action = actionNames[random.nextInt(actionNames.length)];

        userLogBuffer.append(dateToDay+"\t")
            .append(timestamp + "\t")
                .append(userId + "\t")
                .append(pageId + "\t")
                .append(channel + "\t")
                .append(action);

        return userLogBuffer.toString();
    }

    public static void main(String[] args) {
        //UserLogs topic
        SparkStreamingDataManuallyProducer4Kafka generatorData = new SparkStreamingDataManuallyProducer4Kafka("UserLogs");
        try {
            Thread thread = new Thread(generatorData);
            thread.start();
            thread.join();
        } catch (Exception e) {
            e.printStackTrace();
            System.exit(-1);
        }
    }
}
```

##代码实战

功能说明：

* 在线PV计算
* 在线UV计算
* 在线计算注册人数
* 在线计算跳出率
* 在线计算不同模块的PV

OnlineBBSUserLogs.scala

```scala
package com.dtspark.sparkapps.streaming;

import kafka.serializer.StringDecoder;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.*;
import org.apache.spark.streaming.kafka.KafkaUtils;
import scala.Tuple2;

import java.util.*;

/**
 * 第98课：计算页面的PV
 */
public class OnlineBBSUserLogs {
    public static void main(String[] args) {
        SparkConf conf = new SparkConf().setAppName("OnlineBBSUserLogs");
        conf.setMaster("local[4]");

        JavaStreamingContext jsc = new JavaStreamingContext(conf, Durations.seconds(5));
        jsc.sparkContext().setLogLevel("ERROR");

        Map<String,String> kafkaParameters = new HashMap();
        kafkaParameters.put("metadata.broker.list", "localhost:9092");
        Set<String> topicSet = new HashSet<>();
        topicSet.add("UserLogs");

        JavaPairInputDStream<String,String> lines = KafkaUtils.createDirectStream(jsc,
                String.class, String.class, StringDecoder.class, StringDecoder.class, kafkaParameters, topicSet);

//        Map<String,Integer> topics = new HashMap();
//        topics.put("UserLogs", 1);
//        JavaPairReceiverInputDStream<String,String> lines = KafkaUtils.createStream(jsc,"localhost:2181","BBS",topics);

        //在线PV计算
        onlinePV(lines);
        //在线UV计算
        onlineUV(lines);
        //在线计算注册人数
        onLineRegistered(lines);
        //在线计算跳出率
        onLineJumped(lines);
        //在线计算不同模块的PV
        onLineChannelPV(lines);

        jsc.start();

        jsc.awaitTermination();
    }

    private static void onLineChannelPV(JavaPairInputDStream<String, String> lines) {
        JavaPairDStream<String,Long> channelsDStream = lines.mapToPair(line->{
            String [] cols = line._2().split("\t");
            return new Tuple2(cols[4],1L);
        });
        channelsDStream.reduceByKey((v1,v2)->v1+v2).print();
    }

    /**
     * 在线计算跳出率
     * @param lines
     */
    private static void onLineJumped(JavaPairInputDStream<String, String> lines) {
        JavaPairDStream<String,Integer> userCountDStream = lines.filter(tuple2 -> {
            String[] logs = tuple2._2().split("\t");
            String action = logs[5];
            if ("View".equals(action)) {
                return true;
            } else {
                return false;
            }
        }).mapToPair(tuple2 -> {
            String[] logs = tuple2._2().split("\t");
            String userID = logs[2];
            return new Tuple2(userID, 1);
        });
        userCountDStream.reduceByKey((v1, v2) -> {
            return v1 + v2;
        }).filter((tuple2) -> {
            if (1 == tuple2._2() ) {
                return true;
            } else {
                return false;
            }
        }).count().print();
    }

    private static void onLineRegistered(JavaPairInputDStream<String, String> lines) {
        lines.filter(tuple2 -> {
            String[] logs = tuple2._2().split("\t");
            String action = logs[5];
            if ("Register".equals(action)) {
                return true;
            } else {
                return false;
            }
        }).count().print();
    }

    /**
     * 因为要计算UV，所以需要获得同样的Page的不同的User，这个时候就需要去重操作。
     *  DStream中有distinct吗？当然没有（截止到Spark 1.6.1的时候还没有该API）
     *  此时我们就需要求助于DStream魔术般的transform，在该方法内部直接对RDD进行distinct操作，
     *  这样就实现了UserID的去重，进而就可以计算出UV了。
     * @param lines
     */
    private static void onlineUV(JavaPairInputDStream<String, String> lines) {
        //过滤掉非View的action
        JavaPairDStream<String,String> logsDStream = lines.filter(tuple2 -> {
            String[] logs = tuple2._2().split("\t");
            String action = logs[5];
            if ("View".equals(action)) {
                return true;
            } else {
                return false;
            }
        });
        JavaPairDStream result = logsDStream.map(tuple2 -> {
            String[] logs = tuple2._2().split("\t");
            Long userID = Long.parseLong(logs[2]);
            Long pageID = Long.parseLong(logs[3]);
            return pageID + "_" + userID;
        }).transform((JavaRDD<String> rdd) -> {
            return rdd.distinct();
        }).mapToPair((String t) -> {
            String[] cols = t.split("_");
            return new Tuple2(cols[0], 1);
        });
        /**
         * 在企业生产环境下，一般会把计算的数据放入Redis或者DB中，采用J2EE等技术进行趋势的绘制等，
         *  这就像动态更新的股票交易一样来实现在线的监控等。
         */
        //JavaPairDStream<String,Integer> result2 = result;
        result.print();
    }

    /**
     * 在线PV计算
     * @param lines
     */
    public static void onlinePV(JavaPairInputDStream<String, String> lines) {
        /**
         * 计算页面的PV，过滤掉非View的action
         */
        JavaPairDStream<String,String> logsDStream = lines.filter(tuple2 -> {
            String[] logs = tuple2._2().split("\t");
            String action = logs[5];
            if ("View".equals(action)) {
                return true;
            } else {
                return false;
            }
        });
        JavaPairDStream<Long,Long> pairs = logsDStream.mapToPair(tuple2->{
            String [] logs = tuple2._2().split("\t");
            Long pageId = Long.parseLong(logs[3]);
            return new Tuple2<Long, Long>(pageId,1L);
        });

        JavaPairDStream<Long,Long> wordsCount = pairs.reduceByKey((v1, v2) -> v1 + v2);
        /**
         * 在企业生产环境下，一般会把计算的数据放入Redis或者DB中，采用J2EE等技术进行趋势的绘制等，
         *  这就像动态更新的股票交易一样来实现在线的监控等。
         */
        wordsCount.print();
    }
}
```

##项目测试流程

* 启动Kafka-Zookeeper
```sh
zookeeper-server-start G:/runtime/kafka_2.11-0.10.0.0/config/zookeeper.properties
```
* 启动Kafka
```sh
kafka-server-start G:/runtime/kafka_2.11-0.10.0.0/config/server.properties
```
* 生成Topic
```sh
kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic UserLogs
```
* 测试Topic
```sh
kafka-topics --list --zookeeper localhost:2181
```
* 运行SparkStreamingDataManuallyProducer4Kafka.java
* 查看数据是否生成成功
```sh
kafka-console-consumer --zookeeper localhost:2181 --topic UserLogs --from-beginning
```
* 运行OnlineBBSUserLogs.java

##作业：把java代码用scala重写

