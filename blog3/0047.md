# 第47课：王家林谈Spark性能优化第三季！

标签： sparkIMF

---

##一：Spark程序数据结构的优化

 1. Java的对象：对象头是16个字节（例如指向对象的指针等元数据信息），如果对象中只有一个整数的property，则此时会占据20个字节，也就是说对象的元数据占用了大部分的空间，所以在封装数据的时候尽量不要使用对象！例如说使用JSON格式的来封装数据；
 2. Java中基本的数据类型会自动的封箱操作，例如int会自动变成Integer，这会额外增加对象头的空间占用；
 3. Java的String在实际占用内存方面要额外使用40个字节（String的内部使用的是char[]来保存字符序列），另外需要注意的是String中每个字符是2个字节（UTF-16），如果内部有5个字符的话，实际上会占用50个字节；
 4. Java中的集合List、HashMap等等其内部一般使用链表来实现，具体的每个数据是使用Entry等，这些也非常消耗内存；
 5. 优先使用原生数组，尽可能不要直接去使用ArrayList、HashMap、LinkedList等数据结构，例如说List<Integer> list = new ArrayList<Integer>()需要考虑替换为int[]array = new int[];
 6. 优先使用String（推荐使用JSON），而不是采用HashMap、List等封装数据，例如Map<Integer,Worker> workers = new HashMap<Integer,WOrker>()，建议使用JSON字符串或者自己构建的String字符串对象，例如“id:name,salary||id:name,salary||id:name,salary”。

##二：Spark内存消耗诊断

 1. JVM自带众多内存消耗诊断的工具，例如JMap、JConsole等，第三方IBM JVM Profile Tools等；
 2. 在开发、测试、生产环境下用的最多的是日志？！Driver产生的日志！！！最简单也是最有效的方式就是调用RDD.cache，当进行cache操作的时候Driver上的BlockManagerMaster会记录该信息并写进日志中！

##三：persist和checkpoint

 1. 当反复使用某个（些）RDD的时候强烈建议使用persist来对数据进行缓存（MEMORY_AND_DISK）。
 2. 如果某个步骤的RDD计算特别耗时或者经历了很多步骤的计算，如果数据丢失则重新计算的代价特别大，此时考虑使用checkpoint，因为checkpoint是把数据写入HDFS，天然具有高可靠性。

##作业：使用cache诊断内存消耗！
